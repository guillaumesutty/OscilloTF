{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ceb3c3-687f-4d87-9787-94f8989e31a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coo_matrix\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr, pearsonr\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils_plot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils_plot'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from itertools import product\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from utils_plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e66cd-8667-4e39-ac45-1f50489cde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Normalization\n",
    "def preprocessing(alpha1, alpha2, N, ampl_threshold=0.2):\n",
    "\n",
    "    targetnames = np.array(sorted(set(N.index) & set(alpha1.index) & set(alpha2.index)))\n",
    "    print(\"Genes in common :\", len(targetnames))\n",
    "\n",
    "    alpha1, alpha2 = alpha1.loc[targetnames].to_numpy(), alpha2.loc[targetnames].to_numpy()\n",
    " \n",
    "    ampl1 = (alpha1.max(axis=1)-alpha1.min(axis=1))/2\n",
    "    ampl2 = (alpha2.max(axis=1)-alpha2.min(axis=1))/2\n",
    "    ind = (ampl1 > ampl_threshold) & (ampl2 > ampl_threshold)\n",
    "    alpha1, alpha2 = alpha1[ind,:], alpha2[ind,:]\n",
    "    targetnames_filtered = targetnames[ind]\n",
    "\n",
    "    N = N.loc[targetnames_filtered].to_numpy()\n",
    "    \n",
    "    # Identify TFs that are not present in any gene\n",
    "    inactive_tfs = np.where(N.sum(axis=0) == 0)[0]\n",
    "    print(f\"Number of inactive TFs: {len(inactive_tfs)}\")\n",
    "    N = np.delete(N, inactive_tfs, axis=1)\n",
    "    tf_names_filtered = np.delete(tf_names, inactive_tfs)\n",
    "    \n",
    "    print(f\"Kept genes: {N.shape[0]} (ampl > {ampl_threshold})\")\n",
    "    alpha1_norm = alpha1 - np.mean(alpha1, axis=1, keepdims=True) - np.mean(alpha1, axis=0, keepdims=True) + np.mean(alpha1)\n",
    "    alpha2_norm = alpha2 - np.mean(alpha2, axis=1, keepdims=True) - np.mean(alpha2, axis=0, keepdims=True) + np.mean(alpha2)\n",
    "    #N_norm = N - np.mean(N, axis=0, keepdims=True) #We will optimize the sparse matrix, so we need to keep the absolute zero values.\n",
    "\n",
    "    return alpha1_norm, alpha2_norm, N, targetnames_filtered, tf_names_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738bd4a-c8e7-458b-9bd4-a7a40407c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Define Ridge Regression Model with Trainable Sparse W\n",
    "class TrainableModel(nn.Module):\n",
    "    def __init__(self, N, alpha, num_tfs, num_thetas, lambda1=0.01, lambda2=0.01):\n",
    "        super(TrainableModel, self).__init__()\n",
    "        \n",
    "        self.lambda1 = lambda1  # L1 regularization for W\n",
    "        self.lambda2 = lambda2  # L2 regularization for A\n",
    "\n",
    "        # Convert N to COO format\n",
    "        sparse_matrix = coo_matrix(N)\n",
    "\n",
    "        # Get the nonzero indices and values\n",
    "        self.i = torch.tensor(sparse_matrix.row, dtype=torch.long)\n",
    "        self.j = torch.tensor(sparse_matrix.col, dtype=torch.long)\n",
    "        values = torch.tensor(sparse_matrix.data, dtype=torch.float32)\n",
    "\n",
    "        # Create W as a trainable vector for the non-zero elements of N\n",
    "        self.W = nn.Parameter(torch.randn(len(values)) * 0.05)\n",
    "\n",
    "        # Initialize the unconstrained TF activity parameters\n",
    "        self.x = nn.Parameter(torch.randn(num_tfs, num_thetas) * 0.05)\n",
    "\n",
    "        self.num_genes, self.num_tfs = N.shape\n",
    "\n",
    "    def forward(self):\n",
    "        # Create a sparse tensor for W\n",
    "        W_sparse = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([self.i, self.j]), \n",
    "            values=self.W, \n",
    "            size=(self.num_genes, self.num_tfs)\n",
    "        )\n",
    "        W_dense_tensor = W_sparse.to_dense()\n",
    "\n",
    "        # Constrain A to be between 0 and 1 using the sigmoid function.\n",
    "        A = torch.sigmoid(self.x)\n",
    "\n",
    "        # Compute the reconstructed alpha matrix.\n",
    "        return torch.matmul(W_dense_tensor, A)\n",
    "\n",
    "    def loss(self, alpha_true):\n",
    "        alpha_pred = self.forward()\n",
    "        main_loss = torch.sum((alpha_true - alpha_pred) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(self.W))  # L1 on W\n",
    "        l2_loss = torch.sum(self.x ** 2)         # L2 on the unconstrained A\n",
    "\n",
    "        total_loss = main_loss + self.lambda1 * l1_loss + self.lambda2 * l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1e904-dfeb-4781-8113-bfc1cd65f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(N, alpha, alpha_test, lambda1, lambda2, patience=20, num_epochs=1000, lr=0.005):\n",
    "    num_genes, num_tfs = N.shape\n",
    "    num_thetas = alpha.shape[1]\n",
    "    \n",
    "    model = TrainableModel(N, alpha, num_tfs, num_thetas, lambda1, lambda2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_EV = -float(\"inf\")\n",
    "    best_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       # Validation Step (keep computations in PyTorch)\n",
    "        W_sparse_vector = model.W.detach()\n",
    "        W_dense = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([model.i, model.j]),\n",
    "            values=W_sparse_vector,\n",
    "            size=(num_genes, num_tfs)\n",
    "        ).to_dense()\n",
    "        x = model.x.detach()\n",
    "        A = torch.sigmoid(x)\n",
    "\n",
    "        # Calculate R_test as a torch tensor\n",
    "        R_test = torch.matmul(W_dense, A)\n",
    "\n",
    "        # Calculate EV_test (convert tensors to NumPy for explained_variance_score)\n",
    "        EV_test = explained_variance_score(alpha_test.numpy(), R_test.detach().numpy())\n",
    "\n",
    "        # Calculate Test Loss in PyTorch\n",
    "        main_loss = torch.sum((alpha_test - R_test) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(W_sparse_vector))  # L1 on W\n",
    "        l2_loss = torch.sum(x ** 2)                         # L2 on the unconstrained A\n",
    "        total_test_loss = main_loss + lambda1 * l1_loss + lambda2 * l2_loss\n",
    "\n",
    "        # Check for early stopping\n",
    "        if total_test_loss < best_test_loss-25:\n",
    "            best_EV = EV_test\n",
    "            best_test_loss = total_test_loss\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.0f}, Loss test: {total_test_loss:.0f}, EV_test: {EV_test*100:.2f}%\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            break\n",
    "\n",
    "    return W_dense.numpy(), A.numpy(), x.numpy(), loss.item()#, total_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538973c-9628-4f67-8881-5642595f3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cross-Validation for Lambda Optimization\n",
    "def cross_val_lambda(N, alpha1, alpha2, lambda1_values, lambda2_values):\n",
    "    best_lambda1, best_lambda2, best_EV = None, None, -np.inf\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    EVs_avg = []\n",
    "\n",
    "    for lambda1, lambda2 in product(lambda1_values, lambda2_values):\n",
    "        print(f\"Testing lambda1 = {lambda1:.2f}, lambda2 = {lambda2:.2f}\")\n",
    "\n",
    "        # Train on alpha1, test on alpha2\n",
    "        W1, A1, x1, loss1 = train_model(N, alpha1, alpha2, lambda1, lambda2)\n",
    "        losses1.append(loss1)\n",
    "        R_test1 = W1 @ A1\n",
    "        EV1 = explained_variance_score(alpha2, R_test1)\n",
    "\n",
    "        # Train on alpha2, test on alpha1\n",
    "        W2, A2, x2, loss2 = train_model(N, alpha2, alpha1, lambda1, lambda2)\n",
    "        losses2.append(loss2)\n",
    "        R_test2 = W2 @ A2\n",
    "        EV2 = explained_variance_score(alpha1, R_test2)\n",
    "\n",
    "        avg_EV = (EV1 + EV2) / 2\n",
    "        EVs_avg.append(avg_EV)\n",
    "        print(f\"Lambda1={lambda1:.2f}, Lambda2={lambda2:.2f}, EV={avg_EV*100:.2f}%\\n\")\n",
    "\n",
    "        if avg_EV > best_EV:\n",
    "            best_lambda1, best_lambda2, best_EV = lambda1, lambda2, avg_EV\n",
    "            \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(lambda2_values, EVs_avg)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Lambda1\")\n",
    "    plt.ylabel(\"EV Test\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"EV test vs Lambda\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(lambda2_values, losses1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Lambda1\")\n",
    "    plt.ylabel(\"Loss function\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Loss vs Lambda\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best λ1 = {best_lambda1:.2f}, Best λ2={best_lambda2:.2f}, Best EV={best_EV*100:.2f}%\\n\")\n",
    "    return best_lambda1, best_lambda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0918370-967c-4196-9a19-49d494d0f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross train for best model\n",
    "def cross_train(N, alpha1, alpha2, best_lambda1, best_lambda2):\n",
    "    print(\"Training on α1, testing on α2...\")\n",
    "    W1, A1, x1, loss1 = train_model(N, alpha1, alpha2, best_lambda1, best_lambda2)\n",
    "    R_test1 = W1 @ A1\n",
    "    EV1_train = explained_variance_score(alpha1.numpy(), R_test1)\n",
    "    EV1_test = explained_variance_score(alpha2.numpy(), R_test1)\n",
    "\n",
    "    print(\"Training on α2, testing on α1...\")\n",
    "    W2, A2, x2, loss2 = train_model(N, alpha2, alpha1, best_lambda1, best_lambda2)\n",
    "    R_test2 = W2 @ A2\n",
    "    EV2_train = explained_variance_score(alpha2.numpy(), R_test2)\n",
    "    EV2_test = explained_variance_score(alpha1.numpy(), R_test2)\n",
    "\n",
    "    avg_EV_train = (EV1_train + EV2_train) / 2\n",
    "    avg_EV_test = (EV1_test + EV2_test) / 2\n",
    "    print(f\"Average EV_train: {avg_EV_train*100:.2f}%\")\n",
    "    print(f\"Average EV_test: {avg_EV_test*100:.2f}%\")\n",
    "    \n",
    "    return W1, A1, x1, W2, A2, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a259a-0778-488f-b149-59970090e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_amplitudes(matrices, target_amp=0.2):\n",
    "    \"\"\"\n",
    "    Rescale the matrices to a single target amplitude.\n",
    "    \"\"\"\n",
    "    standardized_matrices = []\n",
    "    for matrix in matrices:\n",
    "        amp = (np.max(matrix, axis=1) - np.min(matrix, axis=1)) / 2\n",
    "        scale = target_amp / amp\n",
    "        standardized_matrix = matrix * scale[:, np.newaxis]\n",
    "        standardized_matrices.append(standardized_matrix)\n",
    "    \n",
    "    return standardized_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42624f9-10c9-4c9b-abd4-c4d2adcf220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data & Run\n",
    "fileAlpha1 = \"/shared/space2/molina/suttyg/alpha_snrna_rep1_5000_1_2p75.csv\"\n",
    "fileAlpha2 = \"/shared/space2/molina/suttyg/alpha_snrna_rep2_5000_1_2p75.csv\"\n",
    "fileBSM = '/shared/space2/molina/suttyg/data_binding_site_matrix.txt'\n",
    "process = [\"transcription\", \"\\u03B1\"]\n",
    "theta_smooth = np.round(np.linspace(0.01, 1.00, 100), 2)  # 100 bins from 0.01 to 1.00\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define Lambda Values\n",
    "#lambda1_values = np.logspace(-2, 1, 4)\n",
    "#lambda2_values = np.logspace(-2, 1, 4)\n",
    "#lambda1_values = [0.89]\n",
    "best_lambda1, best_lambda2 = 0.89, 0.05\n",
    "\n",
    "N = pd.read_csv(fileBSM, sep=\"\\t\",index_col=0)\n",
    "tf_names = N.columns\n",
    "alpha1 = pd.read_csv(fileAlpha1, sep=\",\",index_col=0)\n",
    "alpha2 = pd.read_csv(fileAlpha2, sep=\",\",index_col=0)\n",
    "\n",
    "#Select common genes and normalize\n",
    "print(alpha1.shape, alpha2.shape, N.shape)\n",
    "ampl_threshold=0.2\n",
    "alpha1_norm, alpha2_norm, N_norm, targetnames, tf_names = preprocessing(alpha1, alpha2, N, ampl_threshold=ampl_threshold)\n",
    "print(alpha1_norm.shape, alpha2_norm.shape, N_norm.shape, \"\\n\")\n",
    "\n",
    "N_tensor = torch.tensor(N_norm, dtype=torch.float32)  # (genes, TFs)\n",
    "alpha1_tensor = torch.tensor(alpha1_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "alpha2_tensor = torch.tensor(alpha2_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "\n",
    "# Optimize Lambda\n",
    "#best_lambda1, best_lambda2 = cross_val_lambda(N_tensor, alpha1_tensor, alpha2_tensor, lambda1_values, lambda2_values)\n",
    "\n",
    "# Train and Cross-Test\n",
    "W1, A1, x1, W2, A2, x2 = cross_train(N_tensor, alpha1_tensor, alpha2_tensor, best_lambda1, best_lambda2)\n",
    "print(np.sort(x1.flatten()))\n",
    "print(np.sort(x2.flatten()))\n",
    "\n",
    "# Save activities (TFs × θ)\n",
    "#np.save(\"activities_export/A_star_pytorch.npy\", A1)\n",
    "#np.save(\"activities_export/tf_names_pytorch.npy\", np.array(tf_names))\n",
    "#np.save(\"activities_export/targetnames_pytorch.npy\", np.array(targetnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2d2d8-ca9a-4e86-8f34-f4590dd9d0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.sort(W1.flatten()))\n",
    "print(np.sort(x1.flatten()))\n",
    "print(np.sort(A1.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca290251-fbb1-4774-bcf0-c6ec3842b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We smooth activities\n",
    "A1 = fourier_fit(A1, theta_smooth)\n",
    "A2 = fourier_fit(A2, theta_smooth)\n",
    "R1 = W1 @ A1\n",
    "R2 = W2 @ A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856477d-0341-4f02-a24a-a8efc228bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_nb = np.where(tf_names == 'Hes1')[0][0]\n",
    "#BP_nb = 30\n",
    "plot_binding_protein_activity(tf_names, A1, process, theta_smooth, BP_nb=BP_nb)\n",
    "print(f\"Positive W1 among target genes of {tf_names[BP_nb]} : {np.sum(W1[:, BP_nb] > 0)}/{np.sum(W1[:, BP_nb] != 0)} ({np.sum(W1[:, BP_nb] > 0)/np.sum(W1[:, BP_nb] != 0)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6363ee-7e07-468f-9c69-e4ecfae77c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.where(targetnames == 'Nusap1')[0][0]\n",
    "#n = 4972\n",
    "print(\"Train\")\n",
    "plot_rate_comparison(targetnames, alpha1_norm, R1, process, theta_smooth, target_nb=n)\n",
    "print(\"Test\")\n",
    "plot_rate_comparison(targetnames, alpha2_norm, R1, process, theta_smooth, target_nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052ba94-df26-41ec-a73f-5529eb00443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_A, corrs_E =  compute_reproducibility(A1, A2, alpha1_norm, alpha2_norm, metric=\"TF activities\")\n",
    "corrs_W, corrs_E = compute_reproducibility(W1, W2, alpha1_norm, alpha2_norm, metric=\"W site counts\")\n",
    "corrs_R, corrs_E = compute_reproducibility(R1, R2, alpha1_norm, alpha2_norm, metric=\"Reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafd589-9889-40b9-a774-b3eb36e69736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of BPs activity along cell cycle (Export)\n",
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2_E2f5\", \"E2f3\", \"E2f4\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\", \"Elf3\", \"Tfap4\"]\n",
    "tf_displayed = plot_heatmap_list(A1, tf_names, key_tfs, clip=True)\n",
    "print(tf_displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6f18b-5998-433c-8b56-25c63496160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_activity = {\n",
    "    \"Smad3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    \"Hbp1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    \"E2f1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f2_E2f5\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f4\": {\"ranges\": [(0.01, 0.25)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f6\": {\"ranges\": [(0.01, 0.25), (0.63, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f7\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f8\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"Sp1\": {\"ranges\": [(0.1, 0.63)], \"inhibitory\": False},\n",
    "    \"Hes1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    #\"Elf3\": {\"ranges\": [(0.25, 1)], \"inhibitory\": False},\n",
    "    #\"Tfap4\": {\"ranges\": [(0.63, 0.9)], \"inhibitory\": False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77982-f4ab-416c-903c-d232697c7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF EXPRESSION AND BIOLOGICAL MEANING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af104-51b4-4141-8bc8-7f8d57e71317",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2\", \"E2f3\", \"E2f4\", \"E2f5\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\"]\n",
    "\n",
    "fileAlphaTF_sn1 = \"/shared/space2/molina/suttyg/alpha_snrna_rep1_5000_1_2p75.csv\"\n",
    "fileAlphaTF_sn2 = \"/shared/space2/molina/suttyg/alpha_snrna_rep2_5000_1_2p75.csv\"\n",
    "\n",
    "#Get spliced RNA data to compare splicing and protein activity dynamics\n",
    "alpha_sn1 = pd.read_csv(fileAlphaTF_sn1, sep=\",\",index_col=0)\n",
    "alpha_sn2 = pd.read_csv(fileAlphaTF_sn2, sep=\",\",index_col=0)\n",
    "\n",
    "tf_names_filtered = np.array([tf for tf in key_tfs if tf in alpha_sn1.index and tf in alpha_sn2.index and tf in tf_names])\n",
    "print(\"TFs in common :\", str(len(tf_names_filtered))+\"/\"+str(len(key_tfs)))\n",
    "\n",
    "alpha_sn1_f, alpha_sn2_f = alpha_sn1.loc[tf_names_filtered], alpha_sn2.loc[tf_names_filtered]\n",
    "alpha_sn1_n, alpha_sn2_n = alpha_sn1_f.to_numpy(), alpha_sn2_f.to_numpy()\n",
    "\n",
    "#Standardize amplitudes\n",
    "#A_standard = A_standard - np.mean(A_standard, axis=1, keepdims=True)\n",
    "alpha_sn_n = (alpha_sn1_n + alpha_sn2_n) / 2\n",
    "alpha_sn_norm = alpha_sn_n - np.mean(alpha_sn_n, axis=1, keepdims=True) - np.mean(alpha_sn_n, axis=0, keepdims=True) + np.mean(alpha_sn_n)\n",
    "alpha_sn_norm, A_standard = standardize_amplitudes([alpha_sn_norm, A1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55373359-4bd7-4b2a-a93a-b5ef3d62370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "z_vals = []\n",
    "for tf in range(len(tf_names_filtered)):\n",
    "    plot_TF_exp_activity(theta_smooth, alpha_sn_norm, A_standard, tf_names, tf_names_filtered, tf)\n",
    "    corr = spearmanr(alpha_sn_norm[tf], A_standard[list(tf_names).index(tf_names_filtered[tf])])[0]\n",
    "    action = \"activator\" if not expected_activity[tf_names_filtered[tf]][\"inhibitory\"] else \"inhibitor\"\n",
    "    if (action == \"inhibitor\"):\n",
    "        corr = -corr\n",
    "    print(f\"scRNA & A correlation : {corr:.3f} ({ action })\\n\")\n",
    "    z_val = compute_tf_activity_difference(A_standard[list(tf_names).index(tf_names_filtered[tf]), :], theta_smooth, expected_activity[tf_names_filtered[tf]][\"ranges\"], expected_activity[tf_names_filtered[tf]][\"inhibitory\"])\n",
    "    corrs.append(corr)\n",
    "    z_vals.append(z_val)\n",
    "    print(f\"Expected activity range : {expected_activity[tf_names_filtered[tf]][\"ranges\"]}\")\n",
    "    print(f\"TF activity biological z-score : {z_val:.2f} ({ action })\")\n",
    "print(f\"Global correlation :{np.mean(corrs):.3f}\")\n",
    "print(f\"Global z-score :{np.mean(z_vals):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main 3.12.9",
   "language": "python",
   "name": "main_3_12_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

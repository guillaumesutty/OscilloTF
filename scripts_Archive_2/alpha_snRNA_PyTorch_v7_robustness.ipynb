{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ceb3c3-687f-4d87-9787-94f8989e31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from itertools import product\n",
    "from scipy.sparse import coo_matrix\n",
    "from utils_plot import *\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb5e66cd-8667-4e39-ac45-1f50489cde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Normalization\n",
    "def preprocessing(alpha1, alpha2, N, ampl_threshold=0):\n",
    "\n",
    "    targetnames = np.array(list(set(N.index) & set(alpha1.index) & set(alpha2.index)))\n",
    "    print(\"Genes in common :\", len(targetnames))\n",
    "\n",
    "    alpha1, alpha2 = alpha1.loc[targetnames].to_numpy(), alpha2.loc[targetnames].to_numpy()\n",
    " \n",
    "    ampl1 = (alpha1.max(axis=1)-alpha1.min(axis=1))/2\n",
    "    ampl2 = (alpha2.max(axis=1)-alpha2.min(axis=1))/2\n",
    "    ind = (ampl1 > ampl_threshold) & (ampl2 > ampl_threshold)\n",
    "    alpha1, alpha2 = alpha1[ind,:], alpha2[ind,:]\n",
    "    targetnames_filtered = targetnames[ind]\n",
    "\n",
    "    N = N.loc[targetnames_filtered].to_numpy()\n",
    "    \n",
    "    # Identify TFs that are not present in any gene\n",
    "    inactive_tfs = np.where(N.sum(axis=0) == 0)[0]\n",
    "    print(f\"Number of inactive TFs: {len(inactive_tfs)}\")\n",
    "    N = np.delete(N, inactive_tfs, axis=1)\n",
    "    tf_names_filtered = np.delete(tf_names, inactive_tfs)\n",
    "    \n",
    "    print(f\"Kept genes: {N.shape[0]} (ampl > {ampl_threshold})\")\n",
    "    alpha1_norm = alpha1 - np.mean(alpha1, axis=1, keepdims=True) - np.mean(alpha1, axis=0, keepdims=True) + np.mean(alpha1)\n",
    "    alpha2_norm = alpha2 - np.mean(alpha2, axis=1, keepdims=True) - np.mean(alpha2, axis=0, keepdims=True) + np.mean(alpha2)\n",
    "    #N_norm = N - np.mean(N, axis=0, keepdims=True) #We will optimize the sparse matrix\n",
    "\n",
    "    return alpha1_norm, alpha2_norm, N, targetnames_filtered, tf_names_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8738bd4a-c8e7-458b-9bd4-a7a40407c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Define Ridge Regression Model with Trainable Sparse W\n",
    "class TrainableModel(nn.Module):\n",
    "    def __init__(self, N, alpha, num_tfs, num_thetas, lambda1=0.01, lambda2=0.01):\n",
    "        super(TrainableModel, self).__init__()\n",
    "        \n",
    "        self.lambda1 = lambda1  # L1 regularization for W\n",
    "        self.lambda2 = lambda2  # L2 regularization for A\n",
    "\n",
    "        # Convert N to COO format\n",
    "        sparse_matrix = coo_matrix(N)\n",
    "\n",
    "        # Get the nonzero indices and values\n",
    "        self.i = torch.tensor(sparse_matrix.row, dtype=torch.long)\n",
    "        self.j = torch.tensor(sparse_matrix.col, dtype=torch.long)\n",
    "        values = torch.tensor(sparse_matrix.data, dtype=torch.float32)\n",
    "\n",
    "        # Create W as a trainable vector for the non-zero elements of N\n",
    "        self.W = nn.Parameter(torch.randn(len(values)) * 0.05)\n",
    "\n",
    "        # Initialize the unconstrained TF activity parameters\n",
    "        self.x = nn.Parameter(torch.randn(num_tfs, num_thetas) * 0.05)\n",
    "\n",
    "        self.num_genes, self.num_tfs = N.shape\n",
    "\n",
    "    def forward(self):\n",
    "        # Create a sparse tensor for W\n",
    "        W_sparse = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([self.i, self.j]), \n",
    "            values=self.W, \n",
    "            size=(self.num_genes, self.num_tfs)\n",
    "        )\n",
    "        W_dense_tensor = W_sparse.to_dense()\n",
    "\n",
    "        # Constrain A to be between 0 and 1 using the sigmoid function.\n",
    "        A = torch.sigmoid(self.x)\n",
    "\n",
    "        # Compute the reconstructed alpha matrix.\n",
    "        return torch.matmul(W_dense_tensor, A)\n",
    "\n",
    "    def loss(self, alpha_true):\n",
    "        alpha_pred = self.forward()\n",
    "        main_loss = torch.sum((alpha_true - alpha_pred) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(self.W))  # L1 on W\n",
    "        l2_loss = torch.sum(self.x ** 2)         # L2 on the unconstrained A\n",
    "\n",
    "        total_loss = main_loss + self.lambda1 * l1_loss + self.lambda2 * l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d1e904-dfeb-4781-8113-bfc1cd65f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(N, alpha, alpha_test, lambda1, lambda2, patience=20, num_epochs=1000, lr=0.005):\n",
    "    num_genes, num_tfs = N.shape\n",
    "    num_thetas = alpha.shape[1]\n",
    "    \n",
    "    model = TrainableModel(N, alpha, num_tfs, num_thetas, lambda1, lambda2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_EV = -float(\"inf\")\n",
    "    best_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       # Validation Step (keep computations in PyTorch)\n",
    "        W_sparse_vector = model.W.detach()\n",
    "        W_dense = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([model.i, model.j]),\n",
    "            values=W_sparse_vector,\n",
    "            size=(num_genes, num_tfs)\n",
    "        ).to_dense()\n",
    "        x = model.x.detach()\n",
    "        A = torch.sigmoid(x)\n",
    "\n",
    "        # Calculate R_test as a torch tensor\n",
    "        R_test = torch.matmul(W_dense, A)\n",
    "\n",
    "        # Calculate EV_test (convert tensors to NumPy for explained_variance_score)\n",
    "        EV_test = explained_variance_score(alpha_test.numpy(), R_test.detach().numpy())\n",
    "\n",
    "        # Calculate Test Loss in PyTorch\n",
    "        main_loss = torch.sum((alpha_test - R_test) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(W_sparse_vector))  # L1 on W\n",
    "        l2_loss = torch.sum(x ** 2)                         # L2 on the unconstrained A\n",
    "        total_test_loss = main_loss + lambda1 * l1_loss + lambda2 * l2_loss\n",
    "\n",
    "        # Check for early stopping\n",
    "        if total_test_loss < best_test_loss-25:\n",
    "            best_EV = EV_test\n",
    "            best_test_loss = total_test_loss\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.0f}, Loss test: {total_test_loss:.0f}, EV_test: {EV_test*100:.2f}%\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            break\n",
    "\n",
    "    return W_dense.numpy(), A.numpy(), x.numpy(), loss.item()#, total_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9538973c-9628-4f67-8881-5642595f3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cross-Validation for Lambda Optimization\n",
    "def cross_val_lambda(N, alpha1, alpha2, lambda1_values, lambda2_values):\n",
    "    best_lambda1, best_lambda2, best_EV = None, None, -np.inf\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    EVs_avg = []\n",
    "\n",
    "    for lambda1, lambda2 in product(lambda1_values, lambda2_values):\n",
    "        print(f\"Testing lambda1 = {lambda1:.2f}, lambda2 = {lambda2:.2f}\")\n",
    "\n",
    "        # Train on alpha1, test on alpha2\n",
    "        W1, A1, x1, loss1 = train_model(N, alpha1, alpha2, lambda1, lambda2)\n",
    "        losses1.append(loss1)\n",
    "        R_test1 = W1 @ A1\n",
    "        EV1 = explained_variance_score(alpha2, R_test1)\n",
    "\n",
    "        # Train on alpha2, test on alpha1\n",
    "        W2, A2, x2, loss2 = train_model(N, alpha2, alpha1, lambda1, lambda2)\n",
    "        losses2.append(loss2)\n",
    "        R_test2 = W2 @ A2\n",
    "        EV2 = explained_variance_score(alpha1, R_test2)\n",
    "\n",
    "        avg_EV = (EV1 + EV2) / 2\n",
    "        EVs_avg.append(avg_EV)\n",
    "        print(f\"Lambda1={lambda1:.2f}, Lambda2={lambda2:.2f}, EV={avg_EV*100:.2f}%\\n\")\n",
    "\n",
    "        if avg_EV > best_EV:\n",
    "            best_lambda1, best_lambda2, best_EV = lambda1, lambda2, avg_EV\n",
    "            \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(lambda2_values, EVs_avg)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Lambda1\")\n",
    "    plt.ylabel(\"EV Test\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"EV test vs Lambda\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(lambda2_values, losses1)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Lambda1\")\n",
    "    plt.ylabel(\"Loss function\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Loss vs Lambda\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best λ1 = {best_lambda1:.2f}, Best λ2={best_lambda2:.2f}, Best EV={best_EV*100:.2f}%\\n\")\n",
    "    return best_lambda1, best_lambda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0918370-967c-4196-9a19-49d494d0f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross train for best model\n",
    "def cross_train(N, alpha1, alpha2, best_lambda1, best_lambda2):\n",
    "    print(\"Training on α1, testing on α2...\")\n",
    "    W1, A1, x1, loss1 = train_model(N, alpha1, alpha2, best_lambda1, best_lambda2)\n",
    "    R_test1 = W1 @ A1\n",
    "    EV1 = explained_variance_score(alpha2.numpy(), R_test1)\n",
    "\n",
    "    print(\"Training on α2, testing on α1...\")\n",
    "    W2, A2, x2, loss2 = train_model(N, alpha2, alpha1, best_lambda1, best_lambda2)\n",
    "    R_test2 = W2 @ A2\n",
    "    EV2 = explained_variance_score(alpha1.numpy(), R_test2)\n",
    "\n",
    "    avg_EV = (EV1 + EV2) / 2\n",
    "    print(f\"Average EV: {avg_EV*100:.2f}%\")\n",
    "    \n",
    "    return W1, A1, x1, W2, A2, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43996e89-461f-4ea2-9c5f-fc5dad47eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_basis(theta, num_harmonics=3):\n",
    "    \"\"\"\n",
    "    Generates a Fourier basis matrix using sine and cosine components\n",
    "    \"\"\"\n",
    "    basis = np.zeros((2*num_harmonics,len(theta)))\n",
    "    for i in range(num_harmonics):\n",
    "        basis[2*i,:] = np.cos(2*(i+1)*np.pi*(theta))\n",
    "        basis[2*i+1,:] = np.sin(2*(i+1)*np.pi*(theta))\n",
    "\n",
    "    return basis\n",
    "def fourier_fit(data, num_harmonics=3):\n",
    "    fourier_matrix = fourier_basis(theta_smooth, num_harmonics)  # Shape: (2*num_harmonics+1, 100)\n",
    "\n",
    "    # Solve for all TFs using least squares\n",
    "    params, _, _, _ = np.linalg.lstsq(fourier_matrix.T, data.T, rcond=None)  # Shape: (2*num_harmonics+1, TFs)\n",
    "\n",
    "    # Compute A_smooth for all TFs\n",
    "    data_smooth = params.T @ fourier_matrix  # Shape: (TFs, 100)\n",
    "    \n",
    "    return data_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209a259a-0778-488f-b149-59970090e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_amplitudes(matrices, target_amp=0.2):\n",
    "    \"\"\"\n",
    "    Rescale the matrices to a single target amplitude.\n",
    "    \"\"\"\n",
    "    standardized_matrices = []\n",
    "    for matrix in matrices:\n",
    "        amp = (np.max(matrix, axis=1) - np.min(matrix, axis=1)) / 2\n",
    "        scale = target_amp / amp\n",
    "        standardized_matrix = matrix * scale[:, np.newaxis]\n",
    "        standardized_matrices.append(standardized_matrix)\n",
    "    \n",
    "    return standardized_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42624f9-10c9-4c9b-abd4-c4d2adcf220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11582, 100) (11495, 100) (13568, 370)\n",
      "Genes in common : 8447\n",
      "Number of inactive TFs: 4\n",
      "Kept genes: 2399 (ampl > 0.1)\n",
      "(2399, 100) (2399, 100) (2399, 366) \n",
      "\n",
      "Training on α1, testing on α2...\n",
      "Epoch 0, Loss: 12691, Loss test: 10380, EV_test: -20.97%\n",
      "Epoch 100, Loss: 4797, Loss test: 5662, EV_test: 39.41%\n",
      "Epoch 200, Loss: 2719, Loss test: 5000, EV_test: 58.00%\n",
      "Epoch 300, Loss: 2417, Loss test: 4862, EV_test: 58.27%\n",
      "Early stopping at epoch 302. Best Loss test: 4885. Best EV_test: 58.28%\n",
      "Training on α2, testing on α1...\n",
      "Epoch 0, Loss: 11908, Loss test: 11060, EV_test: -18.79%\n",
      "Epoch 100, Loss: 4549, Loss test: 6111, EV_test: 38.96%\n",
      "Epoch 200, Loss: 2578, Loss test: 5201, EV_test: 58.23%\n",
      "Epoch 300, Loss: 2295, Loss test: 5003, EV_test: 59.35%\n",
      "Early stopping at epoch 322. Best Loss test: 5001. Best EV_test: 59.35%\n",
      "Average EV: 58.83%\n",
      "Training on α1, testing on α2...\n",
      "Epoch 0, Loss: 12576, Loss test: 10302, EV_test: -19.72%\n",
      "Epoch 100, Loss: 4902, Loss test: 5812, EV_test: 38.52%\n",
      "Epoch 200, Loss: 2745, Loss test: 5027, EV_test: 57.73%\n",
      "Early stopping at epoch 264. Best Loss test: 4962. Best EV_test: 58.14%\n",
      "Training on α2, testing on α1...\n",
      "Epoch 0, Loss: 11645, Loss test: 10886, EV_test: -16.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[32m     40\u001b[39m     torch.manual_seed(i)  \u001b[38;5;66;03m# Set different seed for each run\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     W1, A1, x1, W2, A2, x2 = \u001b[43mcross_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha1_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha2_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lambda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lambda2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     W1_list.append(W1[W1 != \u001b[32m0\u001b[39m])\n\u001b[32m     43\u001b[39m     A1_list.append(A1)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcross_train\u001b[39m\u001b[34m(N, alpha1, alpha2, best_lambda1, best_lambda2)\u001b[39m\n\u001b[32m      6\u001b[39m EV1 = explained_variance_score(alpha2.numpy(), R_test1)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining on α2, testing on α1...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m W2, A2, x2, loss2 = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lambda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lambda2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m R_test2 = W2 @ A2\n\u001b[32m     11\u001b[39m EV2 = explained_variance_score(alpha1.numpy(), R_test2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(N, alpha, alpha_test, lambda1, lambda2, patience, num_epochs, lr)\u001b[39m\n\u001b[32m     14\u001b[39m  loss = model.loss(alpha)\n\u001b[32m     15\u001b[39m  loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m  \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Validation Step (keep computations in PyTorch)\u001b[39;00m\n\u001b[32m     19\u001b[39m  W_sparse_vector = model.W.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/main_3_12_9/lib/python3.12/site-packages/torch/optim/adam.py:425\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    423\u001b[39m exp_avg.lerp_(grad, \u001b[32m1\u001b[39m - device_beta1)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    428\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load Data & Run\n",
    "fileAlpha1 = \"/shared/space2/molina/suttyg/alpha_snrna_rep1_5000_1_2p75.csv\"\n",
    "fileAlpha2 = \"/shared/space2/molina/suttyg/alpha_snrna_rep2_5000_1_2p75.csv\"\n",
    "fileBSM = '/shared/space2/molina/suttyg/data_binding_site_matrix.txt'\n",
    "process = [\"transcription\", \"\\u03B1\"]\n",
    "theta_smooth = np.round(np.linspace(0.01, 1.00, 100), 2)  # 100 bins from 0.01 to 1.00\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define Lambda Values\n",
    "#lambda1_values = np.logspace(-1, 1, 10)\n",
    "#lambda2_values = np.logspace(-2, 0, 10)\n",
    "#lambda1_values = [0.89]\n",
    "best_lambda1, best_lambda2 = 0.89, 0.05\n",
    "\n",
    "N = pd.read_csv(fileBSM, sep=\"\\t\",index_col=0)\n",
    "tf_names = N.columns\n",
    "alpha1 = pd.   read_csv(fileAlpha1, sep=\",\",index_col=0)\n",
    "alpha2 = pd.read_csv(fileAlpha2, sep=\",\",index_col=0)\n",
    "\n",
    "#Select common genes and normalize\n",
    "print(alpha1.shape, alpha2.shape, N.shape)\n",
    "ampl_threshold=0.1\n",
    "alpha1_norm, alpha2_norm, N_norm, targetnames, tf_names = preprocessing(alpha1, alpha2, N, ampl_threshold=ampl_threshold)\n",
    "print(alpha1_norm.shape, alpha2_norm.shape, N_norm.shape, \"\\n\")\n",
    "\n",
    "N_tensor = torch.tensor(N_norm, dtype=torch.float32)  # (genes, TFs)\n",
    "alpha1_tensor = torch.tensor(alpha1_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "alpha2_tensor = torch.tensor(alpha2_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "\n",
    "# Optimize Lambda\n",
    "#best_lambda1, best_lambda2 = cross_val_lambda(N_tensor, alpha1_tensor, alpha2_tensor, lambda1_values, lambda2_values)\n",
    "\n",
    "num_runs = 5\n",
    "W1_list = []\n",
    "A1_list = []\n",
    "W2_list = []\n",
    "A2_list = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    torch.manual_seed(i)  # Set different seed for each run\n",
    "    W1, A1, x1, W2, A2, x2 = cross_train(N_tensor, alpha1_tensor, alpha2_tensor, best_lambda1, best_lambda2)\n",
    "    W1_list.append(W1[W1 != 0])\n",
    "    A1_list.append(A1)\n",
    "    W2_list.append(W2[W2 != 0])\n",
    "    A2_list.append(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280f83e-f4ac-4363-928e-f1a1c81b597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_similarity(matrix_list, matrix=True):\n",
    "    num = len(matrix_list)\n",
    "    similarities = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1, num):\n",
    "            if (matrix):\n",
    "                sim = np.mean(cosine_similarity(matrix_list[i], matrix_list[j]))\n",
    "            else:\n",
    "                sim = cosine_similarity(matrix_list[i].reshape(1, -1), matrix_list[j].reshape(1, -1))[0, 0]\n",
    "            similarities.append(sim)\n",
    "    return np.mean(similarities), np.std(similarities)\n",
    "\n",
    "W1_sim_mean, W1_sim_std = compute_mean_similarity(W1_list, matrix=False)\n",
    "A1_sim_mean, A1_sim_std = compute_mean_similarity(A1_list)\n",
    "W2_sim_mean, W2_sim_std = compute_mean_similarity(W2_list, matrix=False)\n",
    "A2_sim_mean, A2_sim_std = compute_mean_similarity(A2_list)\n",
    "\n",
    "print(f\"W1 similarity: {W1_sim_mean:.3f} ± {W1_sim_std:.3f}\")\n",
    "print(f\"A1 similarity: {A1_sim_mean:.3f} ± {A1_sim_std:.3f}\")\n",
    "print(f\"W2 similarity: {W2_sim_mean:.3f} ± {W2_sim_std:.3f}\")\n",
    "print(f\"A2 similarity: {A2_sim_mean:.3f} ± {A2_sim_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee349-fc89-466a-99fc-d0414a2a473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_correlation(matrix_list):\n",
    "    num = len(matrix_list)\n",
    "    correlations = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1, num):\n",
    "            cor = np.corrcoef(matrix_list[i].flatten(), matrix_list[j].flatten())[0, 1]\n",
    "            correlations.append(cor)\n",
    "    return np.mean(correlations), np.std(correlations)\n",
    "\n",
    "W1_corr_mean, W1_corr_std = compute_mean_correlation(W1_list)\n",
    "A1_corr_mean, A1_corr_std = compute_mean_correlation(A1_list)\n",
    "W2_corr_mean, W2_corr_std = compute_mean_correlation(W2_list)\n",
    "A2_corr_mean, A2_corr_std = compute_mean_correlation(A2_list)\n",
    "\n",
    "print(f\"W1 correlation: {W1_corr_mean:.3f} ± {W1_corr_std:.3f}\")\n",
    "print(f\"A1 correlation: {A1_corr_mean:.3f} ± {A1_corr_std:.3f}\")\n",
    "print(f\"W2 correlation: {W2_corr_mean:.3f} ± {W2_corr_std:.3f}\")\n",
    "print(f\"A2 correlation: {A2_corr_mean:.3f} ± {A2_corr_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2ecf8-ee56-4efa-96e6-c506305706e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = np.sum(W1 > 0, axis=0)\n",
    "neg_counts = np.sum(W1 < 0, axis=0)\n",
    "pos_ratio = np.round(pos_counts/(neg_counts+pos_counts),2)\n",
    "plt.hist(pos_ratio, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2d2d8-ca9a-4e86-8f34-f4590dd9d0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.sort(W1.flatten()))\n",
    "print(np.sort(x1.flatten()))\n",
    "print(np.sort(A1.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856477d-0341-4f02-a24a-a8efc228bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_smooth = fourier_fit(A1)\n",
    "BP_nb = np.where(tf_names == 'E2f4')[0][0]\n",
    "#BP_nb = 190\n",
    "plot_binding_protein_activity(tf_names, A1, process, theta_smooth, BP_nb=BP_nb)\n",
    "plot_binding_protein_activity(tf_names, A1_smooth, process, theta_smooth, BP_nb=BP_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6363ee-7e07-468f-9c69-e4ecfae77c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.where(targetnames == 'Nusap1')[0][0]\n",
    "#n = 4972\n",
    "print(\"Train\")\n",
    "plot_rate_comparison(targetnames, alpha1_tensor, W1 @ A1, process, theta_smooth, target_nb=n)\n",
    "print(\"Test\")\n",
    "plot_rate_comparison(targetnames, alpha2_tensor, W1 @ A1, process, theta_smooth, target_nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafd589-9889-40b9-a774-b3eb36e69736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of BPs activity along cell cycle (Export)\n",
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2_E2f5\", \"E2f3\", \"E2f4\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\", \"Elf3\", \"Tfap4\"]\n",
    "tf_displayed = plot_heatmap_list(A1_smooth, tf_names, key_tfs, clip=True)\n",
    "print(tf_displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6f18b-5998-433c-8b56-25c63496160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_activity = {\n",
    "    \"Smad3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    \"Hbp1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    \"E2f1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f2_E2f5\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f4\": {\"ranges\": [(0.01, 0.25)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f6\": {\"ranges\": [(0.01, 0.25), (0.63, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f7\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"E2f8\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": False}, #May be True\n",
    "    \"Sp1\": {\"ranges\": [(0.1, 0.63)], \"inhibitory\": False},\n",
    "    \"Hes1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    #\"Elf3\": {\"ranges\": [(0.25, 1)], \"inhibitory\": False},\n",
    "    #\"Tfap4\": {\"ranges\": [(0.63, 0.9)], \"inhibitory\": False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77982-f4ab-416c-903c-d232697c7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF EXPRESSION AND BIOLOGICAL MEANING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af104-51b4-4141-8bc8-7f8d57e71317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileAlphaTF_sc = \"/shared/space2/molina/suttyg/TF_Expression/spliced_scrna.csv\"\n",
    "fileAlphaTF_sn1 = \"/shared/space2/molina/suttyg/TF_Expression/spliced_snrna_rep1.csv\"\n",
    "fileAlphaTF_sn2 = \"/shared/space2/molina/suttyg/TF_Expression/spliced_snrna_rep2.csv\"\n",
    "\n",
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2\", \"E2f3\", \"E2f4\", \"E2f5\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\"]\n",
    "\n",
    "alpha_sc = pd.read_csv(fileAlphaTF_sc, sep=\",\",index_col=0)\n",
    "alpha_sn1 = pd.read_csv(fileAlphaTF_sn1, sep=\",\",index_col=0)\n",
    "alpha_sn2 = pd.read_csv(fileAlphaTF_sn2, sep=\",\",index_col=0)\n",
    "\n",
    "tf_names_filtered = np.array([tf for tf in key_tfs if tf in alpha_sc.index and tf in alpha_sn1.index and tf in alpha_sn2.index and tf in tf_names])\n",
    "print(\"TFs in common :\", str(len(tf_names_filtered))+\"/\"+str(len(key_tfs)))\n",
    "\n",
    "alpha_sc_f, alpha_sn1_f, alpha_sn2_f = alpha_sc.loc[tf_names_filtered], alpha_sn1.loc[tf_names_filtered], alpha_sn2.loc[tf_names_filtered]\n",
    "alpha_sc_n, alpha_sn1_n, alpha_sn2_n = alpha_sc_f.to_numpy(), alpha_sn1_f.to_numpy(), alpha_sn2_f.to_numpy()\n",
    "\n",
    "#Standardize amplitudes\n",
    "alpha_sc_n, alpha_sn1_n, alpha_sn2_n, A1_standard = standardize_amplitudes([alpha_sc_n, alpha_sn1_n, alpha_sn2_n, A1_smooth])\n",
    "#A1_standard = A1_standard - np.mean(A1_standard, axis=1, keepdims=True)\n",
    "alpha_sn_n = (alpha_sn1_n + alpha_sn2_n) / 2\n",
    "\n",
    "alpha_sc_norm = alpha_sc_n - np.mean(alpha_sc_n, axis=1, keepdims=True) - np.mean(alpha_sc_n, axis=0, keepdims=True) + np.mean(alpha_sc_n)\n",
    "alpha_sn_norm = alpha_sn_n - np.mean(alpha_sn_n, axis=1, keepdims=True) - np.mean(alpha_sn_n, axis=0, keepdims=True) + np.mean(alpha_sn_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55373359-4bd7-4b2a-a93a-b5ef3d62370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "z_vals = []\n",
    "for tf in range(len(tf_names_filtered)):\n",
    "    plot_TF_exp_activity(theta_smooth, alpha_sc_norm, alpha_sn_norm, A1_standard, tf_names, tf_names_filtered, tf)\n",
    "    corr = spearmanr(alpha_sc_norm[tf], A1_standard[list(tf_names).index(tf_names_filtered[tf])])[0]\n",
    "    action = \"activator\" if not expected_activity[tf_names_filtered[tf]][\"inhibitory\"] else \"inhibitor\"\n",
    "    if (action == \"inhibitor\"):\n",
    "        corr = -corr\n",
    "    print(f\"scRNA & A correlation : {corr:.3f} ({ action })\\n\")\n",
    "    z_val = compute_tf_activity_difference(A1_standard[list(tf_names).index(tf_names_filtered[tf]), :], theta_smooth, expected_activity[tf_names_filtered[tf]][\"ranges\"], expected_activity[tf_names_filtered[tf]][\"inhibitory\"])\n",
    "    corrs.append(corr)\n",
    "    z_vals.append(z_val)\n",
    "    print(f\"Expected activity range : {expected_activity[tf_names_filtered[tf]][\"ranges\"]}\")\n",
    "    print(f\"TF activity biological z-score : {z_val:.2f} ({ action })\")\n",
    "    pos_ratio_tf = pos_ratio[list(tf_names).index(tf_names_filtered[tf])]\n",
    "    print(f\"Binding sites positive ratio :{pos_ratio_tf} {\"(activator)\"} \\n\")\n",
    "print(f\"Global correlation :{np.mean(corrs):.3f}\")\n",
    "print(f\"Global z-score :{np.mean(z_vals):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main 3.12.9",
   "language": "python",
   "name": "main_3_12_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

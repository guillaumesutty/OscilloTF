{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ceb3c3-687f-4d87-9787-94f8989e31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import product, combinations\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import networkx as nx\n",
    "from utils_plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e66cd-8667-4e39-ac45-1f50489cde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Normalization\n",
    "def preprocessing(gamma, M, ampl_threshold=0.1):\n",
    "\n",
    "    targetnames = np.array(sorted(set(M.index) & set(gamma.index)))\n",
    "    print(\"Genes in common :\", len(targetnames))\n",
    "\n",
    "    gamma = gamma.loc[targetnames]\n",
    "                           \n",
    "    # Assume gamma has a 'gene' column and expression columns only\n",
    "    # Step 1: Assign replicate indices within each gene\n",
    "    gamma['rep'] = gamma.groupby('gene').cumcount()\n",
    "\n",
    "    # Step 2: Randomly permute replicates per gene\n",
    "    gamma['perm_rep'] = gamma.groupby('gene')['rep'].transform(lambda x: np.random.permutation(len(x)))\n",
    "\n",
    "    # Step 3: Split into two groups\n",
    "    gamma_group1 = gamma[gamma['perm_rep'] < 5].copy()\n",
    "    gamma_group2 = gamma[gamma['perm_rep'] >= 5].copy()\n",
    "\n",
    "    # Step 4: Drop helper columns and compute group means\n",
    "    expr_cols = gamma.columns.difference(['gene', 'rep', 'perm_rep'])\n",
    "\n",
    "    gamma1 = gamma_group1.groupby('gene')[expr_cols].mean().to_numpy()\n",
    "    gamma2 = gamma_group2.groupby('gene')[expr_cols].mean().to_numpy()\n",
    " \n",
    "    ampl1 = (gamma1.max(axis=1)-gamma1.min(axis=1))/2\n",
    "    ampl2 = (gamma2.max(axis=1)-gamma2.min(axis=1))/2\n",
    "    ind = (ampl1 > ampl_threshold) & (ampl2 > ampl_threshold)\n",
    "    gamma1, gamma2 = gamma1[ind,:], gamma2[ind,:]\n",
    "    targetnames_filtered = targetnames[ind]\n",
    "\n",
    "    M = M.loc[targetnames_filtered].to_numpy()\n",
    "    \n",
    "    # Identify TFs that are not present in any gene\n",
    "    inactive_tfs = np.where(M.sum(axis=0) == 0)[0]\n",
    "    print(f\"Number of inactive TFs: {len(inactive_tfs)}\")\n",
    "    M = np.delete(M, inactive_tfs, axis=1)\n",
    "    tf_names_filtered = np.delete(tf_names, inactive_tfs)\n",
    "    \n",
    "    print(f\"Kept genes: {M.shape[0]} (ampl > {ampl_threshold})\")\n",
    "    gamma1_norm = gamma1 - np.mean(gamma1, axis=1, keepdims=True) - np.mean(gamma1, axis=0, keepdims=True) + np.mean(gamma1)\n",
    "    gamma2_norm = gamma2 - np.mean(gamma2, axis=1, keepdims=True) - np.mean(gamma2, axis=0, keepdims=True) + np.mean(gamma2)\n",
    "    #M_norm = M - np.mean(M, axis=0, keepdims=True) #We will optimize the sparse matrix, so we need to keep the absolute zero values.\n",
    "\n",
    "    return gamma1_norm, gamma2_norm, M, targetnames_filtered, tf_names_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae959-bec7-4b39-bdcf-0497d98653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "def svd_regression_with_lambda_CV(gamma, M_BSM, lambdas, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform regression using SVD and select best regularization parameter (lambda)\n",
    "    using k-fold Cross-Validation, following ISMARA approach.\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    G, M = M_BSM.shape\n",
    "    C = gamma.shape[1]\n",
    "    \n",
    "    # Perform SVD once\n",
    "    U, s, VT = np.linalg.svd(M_BSM, full_matrices=False)\n",
    "    \n",
    "    # Initialize k-fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Metrics storage\n",
    "    val_errors = np.zeros((len(lambdas), n_splits))\n",
    "    val_explained_variances = np.zeros((len(lambdas), n_splits))\n",
    "    train_explained_variances = np.zeros((len(lambdas), n_splits))\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(G))):\n",
    "        # Prepare matrices for this fold\n",
    "        U_train = U[train_idx, :]\n",
    "        gamma_train = gamma[train_idx, :]\n",
    "        M_train = U_train.T @ gamma_train\n",
    "        \n",
    "        U_val = U[val_idx, :]\n",
    "        gamma_val = gamma[val_idx, :]\n",
    "        \n",
    "        # Test each lambda value\n",
    "        for i, lambd in enumerate(lambdas):\n",
    "            # Calculate shrinkage factors\n",
    "            shrink = s / (s**2 + lambd)\n",
    "            \n",
    "            # Get A_star for this lambda using training data\n",
    "            A_star = VT.T @ (shrink[:, None] * M_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            R_train = M_BSM[train_idx, :] @ A_star\n",
    "            R_val = M_BSM[val_idx, :] @ A_star\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_errors[i, fold] = mean_squared_error(gamma_val.T, R_val.T)\n",
    "            val_explained_variances[i, fold] = explained_variance_score(gamma_val, R_val)\n",
    "            train_explained_variances[i, fold] = explained_variance_score(gamma_train, R_train)\n",
    "    \n",
    "    # Average metrics across folds\n",
    "    mean_val_errors = np.mean(val_errors, axis=1)\n",
    "    mean_val_explained_variances = np.mean(val_explained_variances, axis=1)\n",
    "    mean_train_explained_variances = np.mean(train_explained_variances, axis=1)\n",
    "    \n",
    "    # Find optimal lambda\n",
    "    best_lambda_idx = np.argmin(mean_val_errors)\n",
    "    lambda_opt = lambdas[best_lambda_idx]\n",
    "    \n",
    "    # Train final model on all data using optimal lambda\n",
    "    shrink_opt = s / (s**2 + lambda_opt)\n",
    "    M_full = U.T @ gamma\n",
    "    A_star = VT.T @ (shrink_opt[:, None] * M_full)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(4,3))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Lambda')\n",
    "    ax1.set_ylabel('Validation MSE', color=color)\n",
    "    ax1.plot(lambdas, mean_val_errors, color=color, label='Validation MSE')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Explained Variance (%)')\n",
    "\n",
    "    color = 'tab:green'\n",
    "    ax2.plot(lambdas, mean_val_explained_variances * 100, color='tab:green', linestyle='--', label='Validation EV')\n",
    "    ax2.plot(lambdas, mean_train_explained_variances * 100, color='tab:orange', linestyle='--', label='Training EV')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    # Legends\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc='best')\n",
    "\n",
    "    fig.suptitle('Cross-Validation: MSE and EV vs Lambda')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return A_star, lambda_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738bd4a-c8e7-458b-9bd4-a7a40407c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: OscilloTF\n",
    "# 2 Define Ridge Regression Model with Trainable Sparse W\n",
    "class TrainableModel(nn.Module):\n",
    "    def __init__(self, M, gamma, A_split, num_tfs, num_thetas, lambdaW=0.01, lambdaA=0.01):\n",
    "        super(TrainableModel, self).__init__()\n",
    "        \n",
    "        self.lambdaW = lambdaW  # L1 regularization for W\n",
    "        self.lambdaA = lambdaA  # L2 regularization for A\n",
    "\n",
    "        # Convert M to COO format\n",
    "        sparse_matrix = coo_matrix(M)\n",
    "\n",
    "        # Get the nonzero indices and values\n",
    "        self.i = torch.tensor(sparse_matrix.row, dtype=torch.long)\n",
    "        self.j = torch.tensor(sparse_matrix.col, dtype=torch.long)\n",
    "        values = sparse_matrix.data\n",
    "\n",
    "        # Create W as a trainable vector for the non-zero elements of M\n",
    "        self.A = nn.Parameter(torch.tensor(A_split, dtype=torch.float32))\n",
    "        self.W = nn.Parameter(torch.tensor(np.log(values), dtype=torch.float32)) #Transform for exponential optimisation M = exp(W) <=> W = log(N)\n",
    "        \n",
    "        self.num_genes, self.num_tfs = M.shape\n",
    "\n",
    "    def forward(self):\n",
    "        # Create a sparse tensor for W\n",
    "        M_sparse = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([self.i, self.j]), \n",
    "            values=torch.exp(self.W),\n",
    "            size=(self.num_genes, self.num_tfs)\n",
    "        )\n",
    "        M_dense_tensor = M_sparse.to_dense()\n",
    "\n",
    "        # Compute the reconstructed gamma matrix.\n",
    "        return torch.matmul(M_dense_tensor, self.A)\n",
    "\n",
    "    def loss(self, gamma_true):\n",
    "        gamma_pred = self.forward()\n",
    "        main_loss = torch.sum((gamma_true - gamma_pred) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(self.W))  # L1 on W\n",
    "        l2_loss = torch.sum(self.A ** 2)         # L2 on A\n",
    "        \n",
    "        #smoothness_loss = 10*torch.sum((self.x[:, 1:] - self.x[:, :-1])**2) #Avoid spikes in A\n",
    "        #cyclic_loss = torch.sum((self.x[:, 0] - self.x[:, -1])**2) #Make activities more cyclic\n",
    "        #slope_loss = torch.sum((self.x[:, 1] - self.x[:, 0] - (self.x[:, -1] - self.x[:, -2]))**2) #Make slopes matc\n",
    "\n",
    "        total_loss = main_loss + self.lambdaW * l1_loss + self.lambdaA * l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1e904-dfeb-4781-8113-bfc1cd65f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(M, gamma, gamma_test, lambdaW, lambdaA, A_split, tol=(0.04, 150), patience=20, num_epochs=400, lr=0.01):\n",
    "    # require 4% improvement every 20 epochs\n",
    "    num_genes, num_tfs = M.shape\n",
    "    num_thetas = gamma.shape[1]\n",
    "    \n",
    "    model = TrainableModel(M, gamma, A_split, num_tfs, num_thetas, lambdaW, lambdaA)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_EV = -float(\"inf\")\n",
    "    best_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    test_EV_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       # Validation Step (keep computations in PyTorch)\n",
    "        W_sparse_vector = model.W.detach()\n",
    "        W_dense = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([model.i, model.j]),\n",
    "            values=torch.exp(W_sparse_vector),\n",
    "            size=(num_genes, num_tfs)\n",
    "        ).to_dense()\n",
    "        A = model.A.detach()\n",
    "\n",
    "        # Calculate R_test as a torch tensor\n",
    "        R_test = torch.matmul(W_dense, A)\n",
    "\n",
    "        # Calculate EV_test (convert tensors to NumPy for explained_variance_score)\n",
    "        EV_test = explained_variance_score(gamma_test.numpy(), R_test.detach().numpy())\n",
    "\n",
    "        # Calculate Test Loss in PyTorch\n",
    "        main_loss = torch.sum((gamma_test - R_test) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(W_sparse_vector))  # L1 on W\n",
    "        l2_loss = torch.sum(A ** 2)                      # L2 on the unconstrained A\n",
    "        total_test_loss = main_loss + lambdaW * l1_loss + lambdaA * l2_loss\n",
    "        \n",
    "        train_loss_history.append(loss.item())\n",
    "        test_loss_history.append(total_test_loss)\n",
    "        test_EV_history.append(EV_test)\n",
    "\n",
    "        # Check for early stopping\n",
    "        is_relative_loss_better = total_test_loss < best_test_loss * (1 - tol[0])\n",
    "        is_absolute_loss_better = total_test_loss < best_test_loss - tol[1]\n",
    "        if is_relative_loss_better and is_absolute_loss_better:\n",
    "            best_EV = EV_test\n",
    "            best_test_loss = total_test_loss\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.0f}, Loss test: {total_test_loss:.0f}, EV_test: {EV_test*100:.2f}%\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            if not is_relative_loss_better:\n",
    "                print(f\"Early stopping (relative={tol[0]}) at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            if not is_absolute_loss_better:\n",
    "                print(f\"Early stopping (absolute={tol[1]}) at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(4, 3))\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss value')\n",
    "    ax1.plot(range(epoch+1), train_loss_history, color='tab:red', label=\"Train Loss\")\n",
    "    ax1.plot(range(epoch+1), test_loss_history, color='tab:green', label=\"Test Loss\")\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc='center right')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('EV Test', color='tab:blue')\n",
    "    ax2.plot(range(epoch+1), test_EV_history, color='tab:blue', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.suptitle('Loss and EV Over Epochs')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return W_dense.numpy(), A.numpy(), loss.item()#, total_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538973c-9628-4f67-8881-5642595f3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cross-Validation for Lambda Optimization\n",
    "def cross_val_lambda(M, gamma1, gamma2, lambdaW_values, lambdaA_values, A_split, A_split_2, tol=(0.04, 150), patience=20):\n",
    "    best_lambdaW, best_lambdaA, best_EV = None, None, -np.inf\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    EVs_avg = []\n",
    "\n",
    "    for lambdaW, lambdaA in product(lambdaW_values, lambdaA_values):\n",
    "        print(f\"Testing lambdaW = {lambdaW:.2f}, lambdaA = {lambdaA:.2f}\")\n",
    "\n",
    "        # Train on gamma1, test on gamma2\n",
    "        W1, A1, loss1 = train_model(M, gamma1, gamma2, lambdaW, lambdaA, A_split, tol, patience)\n",
    "        losses1.append(loss1)\n",
    "        R_test1 = W1 @ A1\n",
    "        EV1 = explained_variance_score(gamma2, R_test1)\n",
    "\n",
    "        # Train on gamma2, test on gamma1\n",
    "        W2, A2, loss2 = train_model(M, gamma2, gamma1, lambdaW, lambdaA, A_split_2, tol, patience)\n",
    "        losses2.append(loss2)\n",
    "        R_test2 = W2 @ A2\n",
    "        EV2 = explained_variance_score(gamma1, R_test2)\n",
    "\n",
    "        avg_EV = (EV1 + EV2) / 2\n",
    "        EVs_avg.append(avg_EV)\n",
    "        print(f\"lambdaW={lambdaW:.2f}, lambdaA={lambdaA:.2f}, EV={avg_EV*100:.2f}%\\n\")\n",
    "\n",
    "        if avg_EV > best_EV:\n",
    "            best_lambdaW, best_lambdaA, best_EV = lambdaW, lambdaA, avg_EV\n",
    "            \n",
    "    EV_surface = np.array(EVs_avg).reshape(len(lambdaW_values), len(lambdaA_values))\n",
    "    LambdaW, LambdaA = np.meshgrid(lambdaW_values, lambdaA_values, indexing='ij')\n",
    "\n",
    "    # Find optimal point\n",
    "    best_idx = np.unravel_index(np.argmax(EV_surface), EV_surface.shape)\n",
    "    opt_lambdaW = lambdaW_values[best_idx[0]]\n",
    "    opt_lambdaA = lambdaA_values[best_idx[1]]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    cp = plt.contourf(LambdaW, LambdaA, EV_surface, levels=30, cmap='viridis')\n",
    "    plt.colorbar(cp, label='Average Explained Variance')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('λ_W (W regularization)')\n",
    "    plt.ylabel('λ_A (A regularization)')\n",
    "    plt.title('EV Surface over λ_W, λ_A')\n",
    "    plt.scatter([opt_lambdaW], [opt_lambdaA], color='red', label='Optimum')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', ls='--', lw=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best λ_W = {best_lambdaW:.4f}, Best λ_A = {best_lambdaA:.2f}, Best EV = {best_EV*100:.2f}%\\n\")\n",
    "    return best_lambdaW, best_lambdaA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0918370-967c-4196-9a19-49d494d0f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross train for best model\n",
    "def cross_train(M, gamma1, gamma2, best_lambdaW, best_lambdaA, A_split, A_split_2, tol=(0.04, 150), patience=20):\n",
    "    print(\"Training on α1, testing on α2...\")\n",
    "    W1, A1, loss1 = train_model(M, gamma1, gamma2, best_lambdaW, best_lambdaA, A_split, tol, patience)\n",
    "    R_test1 = W1 @ A1\n",
    "    EV1_train = explained_variance_score(gamma1.numpy(), R_test1)\n",
    "    EV1_test = explained_variance_score(gamma2.numpy(), R_test1)\n",
    "\n",
    "    print(\"Training on α2, testing on α1...\")\n",
    "    W2, A2, loss2 = train_model(M, gamma2, gamma1, best_lambdaW, best_lambdaA, A_split_2, tol, patience)\n",
    "    R_test2 = W2 @ A2\n",
    "    EV2_train = explained_variance_score(gamma2.numpy(), R_test2)\n",
    "    EV2_test = explained_variance_score(gamma1.numpy(), R_test2)\n",
    "\n",
    "    avg_EV_train = (EV1_train + EV2_train) / 2\n",
    "    avg_EV_test = (EV1_test + EV2_test) / 2\n",
    "    print(f\"Average EV_train: {avg_EV_train*100:.2f}%\")\n",
    "    print(f\"Average EV_test: {avg_EV_test*100:.2f}%\")\n",
    "    \n",
    "    return W1, A1, W2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a259a-0778-488f-b149-59970090e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_amplitudes(matrices_from, matrices_to = False, target_amp=0.2):\n",
    "    \"\"\"\n",
    "    Rescale the matrices to a single target amplitude.\n",
    "    \"\"\"\n",
    "    standardized_matrices_from = []\n",
    "    standardized_matrices_to = []\n",
    "    for i in range(len(matrices_from)):\n",
    "        amp = (np.max(matrices_from[i], axis=1) - np.min(matrices_from[i], axis=1)) / 2\n",
    "        scale = target_amp / amp\n",
    "        standardized_matrix_from = matrices_from[i] * scale[:, np.newaxis] #TF x theta\n",
    "        standardized_matrices_from.append(standardized_matrix_from)\n",
    "        if (matrices_to != False):\n",
    "            standardized_matrix_to = matrices_to[i] / scale[np.newaxis, :] #genes x TF\n",
    "            standardized_matrices_to.append(standardized_matrix_to)\n",
    "    \n",
    "    return standardized_matrices_from, standardized_matrices_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42624f9-10c9-4c9b-abd4-c4d2adcf220e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data & Run\n",
    "fileBSM = 'ENCORI_mm10_RBPTarget_matrix.txt' # text file with the binding sites matrix that infrom the interactions BP -> Target\n",
    "fileGamma = 'bootstrap_gamma_scrna_100_1_1.csv'\n",
    "process = [\"degradation\", \"\\u03B3\"]\n",
    "theta_smooth = np.round(np.linspace(0.01, 1.00, 100), 2)  # 100 bins from 0.01 to 1.00\n",
    "ampl_threshold=0.1\n",
    "\n",
    "# Define Lambda Values\n",
    "lambdaW_values = np.logspace(-3, -2, 5)\n",
    "lambdaA_values = np.logspace(1, 3, 15)\n",
    "#best_lambdaW, best_lambdaA = 0, 0\n",
    "best_lambdaW, best_lambdaA = 0.0056, 372.76\n",
    "\n",
    "M = pd.read_csv(fileBSM, sep=\"\\t\",index_col=0)\n",
    "M = M.drop(M.columns[-1], axis=1)\n",
    "tf_names = M.columns\n",
    "gamma = pd.read_csv(fileGamma, sep=\",\",index_col=0)\n",
    "\n",
    "#Select common genes and normalize\n",
    "print(gamma.shape, M.shape)\n",
    "gamma1_norm, gamma2_norm, M_norm, targetnames, tf_names = preprocessing(gamma, M, ampl_threshold=ampl_threshold)\n",
    "print(gamma1_norm.shape, gamma2_norm.shape, M_norm.shape, \"\\n\")\n",
    "\n",
    "n_runs = 5\n",
    "A1_list = []\n",
    "W1_list = []\n",
    "A2_list = []\n",
    "W2_list = []\n",
    "A1_split_list = []\n",
    "A2_split_list = []\n",
    "\n",
    "for seed in np.random.randint(1,100, n_runs):\n",
    "    torch.manual_seed(seed) #useless now that init is not random anymore\n",
    "    np.random.seed(seed)\n",
    "    lambdas = np.logspace(0, 6, 40)\n",
    "    A_split, lambda_opt1 = svd_regression_with_lambda_CV(gamma1_norm, M_norm, lambdas, seed=seed)\n",
    "    A_split_2, lambda_opt2 = svd_regression_with_lambda_CV(gamma2_norm, M_norm, lambdas, seed=seed)\n",
    "    A1_split_list.append(A_split)\n",
    "    A2_split_list.append(A_split_2)\n",
    "    #print(\"Best lambda:\", lambda_opt1)\n",
    "    #print(\"Best lambda:\", lambda_opt2)\n",
    "\n",
    "    M_tensor = torch.tensor(M_norm, dtype=torch.float32)  # (genes, TFs)\n",
    "    gamma1_tensor = torch.tensor(gamma1_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "    gamma2_tensor = torch.tensor(gamma2_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "\n",
    "    tol = (0.05, 100) #(0.07, 800) for ampl 0.2  #np.linspace(0.01, 0.07, 7)\n",
    "    patience = 20 #np.linspace(5, 30, 7)\n",
    "    # Optimize Lambda\n",
    "    if (best_lambdaW == 0):\n",
    "        best_lambdaW, best_lambdaA = cross_val_lambda(M_tensor, gamma1_tensor, gamma2_tensor, lambdaW_values, lambdaA_values, A_split, A_split_2, tol=(0.04, 150), patience=20)\n",
    "    # Train and Cross-Test\n",
    "    print(\"Seed :\", seed, \"Tolerance :\", tol, \"Patience :\", patience)\n",
    "    W1, A1, W2, A2 = cross_train(M_tensor, gamma1_tensor, gamma2_tensor, best_lambdaW, best_lambdaA, A_split, A_split_2, tol, patience)\n",
    "    A1_list.append(A1)\n",
    "    W1_list.append(W1)\n",
    "    A2_list.append(A2)\n",
    "    W2_list.append(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2388d43-6ca4-4f46-92fc-ca22e4a5bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explv = []\n",
    "for A_spliit in A1_split_list:\n",
    "    R_split = M_norm @ A_spliit\n",
    "    explv.append(explained_variance_score(gamma2_norm, R_split))\n",
    "    #print((explained_variance_score(alpha2_norm, R_split)))\n",
    "for A_spliit in A2_split_list:\n",
    "    R_split = M_norm @ A_spliit\n",
    "    explv.append(explained_variance_score(gamma1_norm, R_split))\n",
    "    #print(explained_variance_score(alpha1_norm, R_split))\n",
    "explv = np.array(explv)\n",
    "print(np.mean(explv)*100)\n",
    "print((np.max(explv)-np.min(explv))/2*100)\n",
    "\n",
    "explv = []\n",
    "for i in range(len(A1_list)):\n",
    "    R_split = W1_list[i] @ A1_list[i]\n",
    "    explv.append(explained_variance_score(gamma2_norm, R_split))\n",
    "    #print((explained_variance_score(alpha2_norm, R_split)))\n",
    "for i in range(len(A2_list)):\n",
    "    R_split = W2_list[i] @ A2_list[i]\n",
    "    explv.append(explained_variance_score(gamma1_norm, R_split))\n",
    "    #print((explained_variance_score(alpha2_norm, R_split)))\n",
    "explv = np.array(explv)\n",
    "print(np.mean(explv)*100)\n",
    "print((np.max(explv)-np.min(explv))/2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41131857-f044-4a90-8749-9ba7a339c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_split = M_norm @ A_split\n",
    "explained_variance_score(gamma1_norm, R_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88b3cc-f178-4827-a6f9-d3bdfe55adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tfs = A1_list[0].shape[0]\n",
    "n_genes = W1_list[0].shape[0]\n",
    "A_list = A1_list\n",
    "W_list = W1_list\n",
    "A_split_list = A1_split_list\n",
    "\n",
    "heatmap_vals = np.zeros((4, n_runs+1, n_runs+1)) #A_split  A1_1  A1_2  A1_3  A1_4  A1_5\n",
    "heatmap_labels = ['Model 1']\n",
    "for l in range(n_runs):\n",
    "    heatmap_labels.append(\"Rep \"+str(l+1))\n",
    "\n",
    "for k in range(n_runs+1): #Diagonal to 1\n",
    "    heatmap_vals[0][k, k] = 1.0\n",
    "    heatmap_vals[1][k, k] = 1.0\n",
    "    heatmap_vals[2][k, k] = 1.0\n",
    "    heatmap_vals[3][k, k] = 1.0\n",
    "\n",
    "for i, j in combinations(range(n_runs), 2): #Corr of A1 between runs\n",
    "    A1_i, A1_j = A_list[i], A_list[j]\n",
    "    W1_i, W1_j = W_list[i], W_list[j]\n",
    "\n",
    "    # Per TF correlation of activities\n",
    "    A_corrs = np.zeros(n_tfs)\n",
    "    for m in range(n_tfs):\n",
    "        A_corrs[m], _ = pearsonr(A1_i[m, :], A1_j[m, :])\n",
    "\n",
    "    # Per gene correlation of weights\n",
    "    W_corrs = np.zeros(n_genes)\n",
    "    for g in range(n_genes):\n",
    "        W_corrs[g], _ = pearsonr(W1_i[g, :], W1_j[g, :])\n",
    "        \n",
    "    heatmap_vals[0][i+1, j+1] = heatmap_vals[0][j+1, i+1] = round(np.mean(A_corrs), 3)\n",
    "    heatmap_vals[1][i+1, j+1] = heatmap_vals[1][j+1, i+1] = round(np.median(A_corrs), 3)\n",
    "    heatmap_vals[2][i+1, j+1] = heatmap_vals[2][j+1, i+1] = round(np.mean(W_corrs), 3)\n",
    "    heatmap_vals[3][i+1, j+1] = heatmap_vals[3][j+1, i+1] = round(np.median(W_corrs), 3)\n",
    "    \n",
    "for l in range(n_runs): #A_split corr with corresponding A after run\n",
    "    A_split, A1 = A_split_list[l], A_list[l]\n",
    "    W1 = W_list[l]\n",
    "\n",
    "    # Per TF correlation of activities\n",
    "    A_corrs = np.zeros(n_tfs)\n",
    "    for m in range(n_tfs):\n",
    "        A_corrs[m], _ = pearsonr(A_split[m, :], A1[m, :])\n",
    "\n",
    "    # Per gene correlation of weights\n",
    "    W_corrs = np.zeros(n_genes)\n",
    "    for g in range(n_genes):\n",
    "        W_corrs[g], _ = pearsonr(M_norm[g, :], W1[g, :])\n",
    "        \n",
    "    heatmap_vals[0][l+1, 0] = heatmap_vals[0][0, l+1] = round(np.mean(A_corrs), 3)\n",
    "    heatmap_vals[1][l+1, 0] = heatmap_vals[1][0, l+1] = round(np.median(A_corrs), 3)\n",
    "    heatmap_vals[2][l+1, 0] = heatmap_vals[2][0, l+1] = round(np.mean(W_corrs), 3)\n",
    "    heatmap_vals[3][l+1, 0] = heatmap_vals[3][0, l+1] = round(np.median(W_corrs), 3)\n",
    "\n",
    "for i, title in enumerate([\"Mean A Corr\", \"Median A Corr\", \"Mean W Corr\", \"Median W Corr\"]):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(heatmap_vals[i], cbar=True, annot=True, fmt=\".2f\", vmin=0, vmax=1, yticklabels=heatmap_labels,xticklabels=heatmap_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2dee58-b83d-4870-9c58-a0cef7e25bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zfp36, Zfp36l1, Elavl1, Upf1, Pabpc1, Ythdf2, Alkbh5\n",
    "BP_nb = np.where(tf_names == 'Zfp36l1')[0][0]\n",
    "plot_binding_protein_activity(tf_names, A_split, process, theta_smooth, BP_nb=BP_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b07ef0-741d-404b-a89d-19eb70e83f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 13,          # base font size\n",
    "    'axes.labelsize': 13,     # axis label font size\n",
    "    'axes.titlesize': 11,     \n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 11,\n",
    "})\n",
    "plt.figure(figsize=(5,5))\n",
    "# Compute TF importance: sum of absolute weights per TF\n",
    "A1_list_standardized, W1_list_standardized = standardize_amplitudes(A1_list, W1_list, target_amp=0.2)\n",
    "print(\"tol=\", tol, \"ampl=\", ampl_threshold, \"Patience=\", patience)\n",
    "W = W1_list_standardized[0]\n",
    "W_key_TF(W, tf_names, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b92475-cdc1-467f-a692-bdec3c7508e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_targets(W, tf_names, gene_names, tf_query):\n",
    "    # Find TF index\n",
    "    tf_idx = np.where(tf_names == tf_query)[0][0]\n",
    "    W_tf = W[:, tf_idx]\n",
    "\n",
    "    # Filter out zero weights\n",
    "    nonzero_idx = np.where(W_tf != 0)[0]\n",
    "    W_tf_nonzero = W_tf[nonzero_idx]\n",
    "    gene_names_nonzero = [gene_names[i] for i in nonzero_idx]\n",
    "\n",
    "    # Sort by absolute weight (descending)\n",
    "    sorted_idx = np.argsort(W_tf_nonzero)[::-1]\n",
    "    top_gene_names = [gene_names_nonzero[i] for i in sorted_idx]\n",
    "    weights = [W_tf_nonzero[i] for i in sorted_idx]\n",
    "    print(str(len(weights))+\" targets for \"+tf_query)\n",
    "\n",
    "    # Build dictionary\n",
    "    targets_dic = dict(zip(top_gene_names, weights))\n",
    "    return targets_dic\n",
    "\n",
    "targets_dic = get_tf_targets(W, tf_names, targetnames, tf_query=\"Ptbp1\")\n",
    "#targets_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2f9d3-a5c4-49ff-b001-ae88cce9281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(W1_list_standardized[0][W1_list_standardized[0] > 0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c657c6c-f6f3-417d-912f-33b93ef2dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, A2 = A1_list[0], A1_list[1]\n",
    "residuals = A1 - A2\n",
    "std_residuals = residuals.std(axis=1)\n",
    "\n",
    "# Compute per-TF Pearson correlations\n",
    "n_tfs = A1.shape[0]\n",
    "pearson_rs = np.array([pearsonr(A1[i, :], A2[i, :])[0] for i in range(n_tfs)])\n",
    "\n",
    "# Plot histogram of std residuals => check for biases\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(std_residuals, bins=30, color='steelblue', alpha=0.8)\n",
    "plt.xlabel(\"Std of Residuals (A1 - A2)\")\n",
    "plt.ylabel(\"Number of TFs\")\n",
    "plt.title(\"Distribution of Std Residuals per TF\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Pearson r vs Residual std => identify shape and/or amplitude disagreements\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(pearson_rs, std_residuals, alpha=0.8, edgecolor='k')\n",
    "plt.xlabel(\"Pearson Correlation (A1 vs A2)\")\n",
    "plt.ylabel(\"Std of Residuals (A1 - A2)\")\n",
    "plt.title(\"Per-TF Reproducibility: Shape vs Magnitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2d2d8-ca9a-4e86-8f34-f4590dd9d0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A1, A2 = A1_list[0], A1_list[1]\n",
    "W1, W2 = W1_list[0], W1_list[1]\n",
    "print(np.sort(W1.flatten()))\n",
    "print(np.sort(A1.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da771a-793d-4189-a616-5bb78c70a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca290251-fbb1-4774-bcf0-c6ec3842b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We smooth activities\n",
    "#A1 = fourier_fit(A1, theta_smooth)\n",
    "#A2 = fourier_fit(A2, theta_smooth)\n",
    "R1 = W1 @ A1\n",
    "R2 = W2 @ A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082e90c-777d-4e7e-9b5f-275d6bcb69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 22,          # base font size\n",
    "    'axes.labelsize': 20,     # axis label font size\n",
    "    'axes.titlesize': 20,     \n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2376f4-88e4-4da3-bedf-e5c7ed5aa5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = []\n",
    "for k in range(gamma1_norm.shape[0]):\n",
    "    expl.append(explained_variance_score(gamma2_norm[k,:], R1[k,:]))\n",
    "expl = np.array(expl)\n",
    "expl_sorted = np.sort(expl)[:]\n",
    "plt.hist(expl_sorted, bins=170)\n",
    "plt.xlabel(\"Explained variance\")\n",
    "plt.xlim(-2,1)\n",
    "plt.ylabel(\"Number of genes\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Distribution of gene-wise EVs for degradation\")\n",
    "plt.show()\n",
    "print(explained_variance_score(gamma2_norm, R1))\n",
    "np.mean(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856477d-0341-4f02-a24a-a8efc228bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_nb = np.where(tf_names == 'Ptbp1')[0][0]\n",
    "#BP_nb = 37\n",
    "plot_binding_protein_activity(tf_names, A1, process, theta_smooth, BP_nb=BP_nb)\n",
    "print(f\"Positive W1 among target genes of {tf_names[BP_nb]} : {np.sum(W1[:, BP_nb] > 0)}/{np.sum(W1[:, BP_nb] != 0)} ({np.sum(W1[:, BP_nb] > 0)/np.sum(W1[:, BP_nb] != 0)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6363ee-7e07-468f-9c69-e4ecfae77c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.where(targetnames == 'Nusap1')[0][0]\n",
    "#n = 4972\n",
    "print(\"Train\")\n",
    "plot_rate_comparison(targetnames, gamma1_norm, R1, process, theta_smooth, target_nb=n)\n",
    "print(\"Test\")\n",
    "plot_rate_comparison(targetnames, gamma2_norm, R1, process, theta_smooth, target_nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052ba94-df26-41ec-a73f-5529eb00443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_reproducibility(A1, A2, gamma1_norm, gamma2_norm, metric=\"TF activities\")\n",
    "compute_reproducibility(W1, W2, gamma1_norm, gamma2_norm, metric=\"W site counts\")\n",
    "#compute_reproducibility(R1, R2, gamma1_norm, gamma2_norm, metric=\"Reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5744d65-34db-4c43-b3b2-b8dea322f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 20,          # base font size\n",
    "    'axes.labelsize': 18,     # axis label font size\n",
    "    'axes.titlesize': 18,     \n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "})\n",
    "A1 = fourier_fit(A1, theta_smooth, num_harmonics=3)\n",
    "plot_heatmap(A1, cmap='RdBu_r', title=\" RBPs activities on degradation across the cell cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafd589-9889-40b9-a774-b3eb36e69736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of BPs activity along cell cycle (Export)\n",
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2_E2f5\", \"E2f3\", \"E2f4\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\", \"Elf3\", \"Tfap4\"]\n",
    "tf_displayed = plot_heatmap_list(A1, tf_names, key_tfs, clip=True)\n",
    "print(tf_displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6f18b-5998-433c-8b56-25c63496160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_activity = {\n",
    "    \"Smad3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False}, #True?\n",
    "    \"Hbp1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    \"E2f1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f2_E2f5\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f3\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": False},\n",
    "    \"E2f4\": {\"ranges\": [(0.01, 0.25)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f6\": {\"ranges\": [(0.01, 0.25), (0.63, 0.9)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f7\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f8\": {\"ranges\": [(0.4, 0.9)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"Sp1\": {\"ranges\": [(0.1, 0.63)], \"inhibitory\": False}, #Can be both?\n",
    "    \"Hes1\": {\"ranges\": [(0.1, 0.4)], \"inhibitory\": True},\n",
    "    #\"Elf3\": {\"ranges\": [(0.25, 1)], \"inhibitory\": False},\n",
    "    #\"Tfap4\": {\"ranges\": [(0.63, 0.9)], \"inhibitory\": False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77982-f4ab-416c-903c-d232697c7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF EXPRESSION AND BIOLOGICAL MEANING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af104-51b4-4141-8bc8-7f8d57e71317",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2\", \"E2f3\", \"E2f4\", \"E2f5\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\"]\n",
    "\n",
    "tf_names_filtered = np.array([tf for tf in key_tfs if tf in gamma.index and tf in M.columns])\n",
    "print(\"TFs in common :\", str(len(tf_names_filtered))+\"/\"+str(len(key_tfs)))\n",
    "\n",
    "gamma_f = gamma.loc[tf_names_filtered]\n",
    "gamma_n = gamma_f.to_numpy()\n",
    "\n",
    "#Standardize amplitudes\n",
    "#A_standard = A_standard - np.mean(A_standard, axis=1, keepdims=True)\n",
    "gamma_n_norm = gamma_n - np.mean(gamma_n, axis=1, keepdims=True) - np.mean(gamma_n, axis=0, keepdims=True) + np.mean(gamma_n)\n",
    "matrices_from, _ = standardize_amplitudes([gamma_n_norm, A1])\n",
    "gamma_n_norm, A_standard = matrices_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55373359-4bd7-4b2a-a93a-b5ef3d62370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "z_vals = []\n",
    "for tf in range(len(tf_names_filtered)):\n",
    "    plot_TF_exp_activity(theta_smooth, gamma_n_norm, A_standard, tf_names, tf_names_filtered, tf)\n",
    "    corr = spearmanr(gamma_n_norm[tf], A_standard[list(tf_names).index(tf_names_filtered[tf])])[0]\n",
    "    action = \"activator\" if not expected_activity[tf_names_filtered[tf]][\"inhibitory\"] else \"inhibitor\"\n",
    "    if (action == \"inhibitor\"):\n",
    "        corr = -corr\n",
    "    print(f\"scRNA & A correlation : {corr:.3f} ({ action })\\n\")\n",
    "    z_val = compute_tf_activity_difference(A_standard[list(tf_names).index(tf_names_filtered[tf]), :], theta_smooth, expected_activity[tf_names_filtered[tf]][\"ranges\"], expected_activity[tf_names_filtered[tf]][\"inhibitory\"])\n",
    "    corrs.append(corr)\n",
    "    z_vals.append(z_val)\n",
    "    print(f\"Expected activity range : {expected_activity[tf_names_filtered[tf]][\"ranges\"]}\")\n",
    "    print(f\"TF activity biological z-score : {z_val:.2f} ({ action })\")\n",
    "print(f\"\\nGlobal correlation :{np.mean(corrs):.3f}\")\n",
    "print(f\"Global z-score :{np.mean(z_vals):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main 3.12.9",
   "language": "python",
   "name": "main_3_12_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ceb3c3-687f-4d87-9787-94f8989e31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import product, combinations\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import networkx as nx\n",
    "from utils_plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e66cd-8667-4e39-ac45-1f50489cde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Normalization\n",
    "def preprocessing(alpha1, alpha2, N, ampl_threshold=0.2):\n",
    "\n",
    "    targetnames = np.array(sorted(set(N.index) & set(alpha1.index) & set(alpha2.index)))\n",
    "    print(\"Genes in common :\", len(targetnames))\n",
    "\n",
    "    alpha1, alpha2 = alpha1.loc[targetnames].to_numpy(), alpha2.loc[targetnames].to_numpy()\n",
    " \n",
    "    ampl1 = (alpha1.max(axis=1)-alpha1.min(axis=1))/2\n",
    "    ampl2 = (alpha2.max(axis=1)-alpha2.min(axis=1))/2\n",
    "    ind = (ampl1 > ampl_threshold) & (ampl2 > ampl_threshold)\n",
    "    alpha1, alpha2 = alpha1[ind,:], alpha2[ind,:]\n",
    "    targetnames_filtered = targetnames[ind]\n",
    "\n",
    "    N = N.loc[targetnames_filtered].to_numpy()\n",
    "    \n",
    "    # Identify TFs that are not present in any gene\n",
    "    inactive_tfs = np.where(N.sum(axis=0) == 0)[0]\n",
    "    print(f\"Number of inactive TFs: {len(inactive_tfs)}\")\n",
    "    N = np.delete(N, inactive_tfs, axis=1)\n",
    "    tf_names_filtered = np.delete(tf_names, inactive_tfs)\n",
    "    \n",
    "    print(f\"Kept genes: {N.shape[0]} (ampl > {ampl_threshold})\")\n",
    "    alpha1_norm = alpha1 - np.mean(alpha1, axis=1, keepdims=True) - np.mean(alpha1, axis=0, keepdims=True) + np.mean(alpha1)\n",
    "    alpha2_norm = alpha2 - np.mean(alpha2, axis=1, keepdims=True) - np.mean(alpha2, axis=0, keepdims=True) + np.mean(alpha2)\n",
    "    #N_norm = N - np.mean(N, axis=0, keepdims=True) #We will optimize the sparse matrix, so we need to keep the absolute zero values.\n",
    "\n",
    "    return alpha1_norm, alpha2_norm, N, targetnames_filtered, tf_names_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae959-bec7-4b39-bdcf-0497d98653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "def svd_regression_with_lambda_CV(alpha, N, lambdas, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform regression using SVD and select best regularization parameter (lambda)\n",
    "    using k-fold Cross-Validation, following ISMARA approach.\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    G, M = N.shape\n",
    "    C = alpha.shape[1]\n",
    "    \n",
    "    # Perform SVD once\n",
    "    U, s, VT = np.linalg.svd(N, full_matrices=False)\n",
    "    \n",
    "    # Initialize k-fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Metrics storage\n",
    "    val_errors = np.zeros((len(lambdas), n_splits))\n",
    "    val_explained_variances = np.zeros((len(lambdas), n_splits))\n",
    "    train_explained_variances = np.zeros((len(lambdas), n_splits))\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(G))):\n",
    "        # Prepare matrices for this fold\n",
    "        U_train = U[train_idx, :]\n",
    "        alpha_train = alpha[train_idx, :]\n",
    "        M_train = U_train.T @ alpha_train\n",
    "        \n",
    "        U_val = U[val_idx, :]\n",
    "        alpha_val = alpha[val_idx, :]\n",
    "        \n",
    "        # Test each lambda value\n",
    "        for i, lambd in enumerate(lambdas):\n",
    "            # Calculate shrinkage factors\n",
    "            shrink = s / (s**2 + lambd)\n",
    "            \n",
    "            # Get A_star for this lambda using training data\n",
    "            A_star = VT.T @ (shrink[:, None] * M_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            R_train = N[train_idx, :] @ A_star\n",
    "            R_val = N[val_idx, :] @ A_star\n",
    "            \n",
    "            # Calculate metrics\n",
    "            val_errors[i, fold] = mean_squared_error(alpha_val.T, R_val.T)\n",
    "            val_explained_variances[i, fold] = explained_variance_score(alpha_val, R_val)\n",
    "            train_explained_variances[i, fold] = explained_variance_score(alpha_train, R_train)\n",
    "    \n",
    "    # Average metrics across folds\n",
    "    mean_val_errors = np.mean(val_errors, axis=1)\n",
    "    mean_val_explained_variances = np.mean(val_explained_variances, axis=1)\n",
    "    mean_train_explained_variances = np.mean(train_explained_variances, axis=1)\n",
    "    \n",
    "    # Find optimal lambda\n",
    "    best_lambda_idx = np.argmin(mean_val_errors)\n",
    "    lambda_opt = lambdas[best_lambda_idx]\n",
    "    \n",
    "    # Train final model on all data using optimal lambda\n",
    "    shrink_opt = s / (s**2 + lambda_opt)\n",
    "    M_full = U.T @ alpha\n",
    "    A_star = VT.T @ (shrink_opt[:, None] * M_full)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(4,3))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Lambda')\n",
    "    ax1.set_ylabel('Validation MSE', color=color)\n",
    "    ax1.plot(lambdas, mean_val_errors, color=color, label='Validation MSE')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Explained Variance (%)')\n",
    "\n",
    "    color = 'tab:green'\n",
    "    ax2.plot(lambdas, mean_val_explained_variances * 100, color='tab:green', linestyle='--', label='Validation EV')\n",
    "    ax2.plot(lambdas, mean_train_explained_variances * 100, color='tab:orange', linestyle='--', label='Training EV')\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    # Legends\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc='best')\n",
    "\n",
    "    fig.suptitle('Cross-Validation: MSE and EV vs Lambda')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return A_star, lambda_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738bd4a-c8e7-458b-9bd4-a7a40407c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: OscilloTF\n",
    "# 2 Define Ridge Regression Model with Trainable Sparse W\n",
    "class TrainableModel(nn.Module):\n",
    "    def __init__(self, N, alpha, A_split, num_tfs, num_thetas, lambdaW=0.01, lambdaA=0.01):\n",
    "        super(TrainableModel, self).__init__()\n",
    "        \n",
    "        self.lambdaW = lambdaW  # L1 regularization for W\n",
    "        self.lambdaA = lambdaA  # L2 regularization for A\n",
    "\n",
    "        # Convert N to COO format\n",
    "        sparse_matrix = coo_matrix(N)\n",
    "\n",
    "        # Get the nonzero indices and values\n",
    "        self.i = torch.tensor(sparse_matrix.row, dtype=torch.long)\n",
    "        self.j = torch.tensor(sparse_matrix.col, dtype=torch.long)\n",
    "        values = sparse_matrix.data\n",
    "\n",
    "        # Create W as a trainable vector for the non-zero elements of N\n",
    "        self.A = nn.Parameter(torch.tensor(A_split, dtype=torch.float32))\n",
    "        self.W = nn.Parameter(torch.tensor(np.log(values), dtype=torch.float32)) #Transform for exponential optimisation N = exp(W) <=> W = log(N)\n",
    "        \n",
    "        self.num_genes, self.num_tfs = N.shape\n",
    "\n",
    "    def forward(self):\n",
    "        # Create a sparse tensor for W\n",
    "        N_sparse = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([self.i, self.j]), \n",
    "            values=torch.exp(self.W),\n",
    "            size=(self.num_genes, self.num_tfs)\n",
    "        )\n",
    "        N_dense_tensor = N_sparse.to_dense()\n",
    "\n",
    "        # Compute the reconstructed alpha matrix.\n",
    "        return torch.matmul(N_dense_tensor, self.A)\n",
    "\n",
    "    def loss(self, alpha_true):\n",
    "        alpha_pred = self.forward()\n",
    "        main_loss = torch.sum((alpha_true - alpha_pred) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(self.W))  # L1 on W\n",
    "        l2_loss = torch.sum(self.A ** 2)         # L2 on A\n",
    "        \n",
    "        #smoothness_loss = 10*torch.sum((self.x[:, 1:] - self.x[:, :-1])**2) #Avoid spikes in A\n",
    "        #cyclic_loss = torch.sum((self.x[:, 0] - self.x[:, -1])**2) #Make activities more cyclic\n",
    "        #slope_loss = torch.sum((self.x[:, 1] - self.x[:, 0] - (self.x[:, -1] - self.x[:, -2]))**2) #Make slopes matc\n",
    "\n",
    "        total_loss = main_loss + self.lambdaW * l1_loss + self.lambdaA * l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1e904-dfeb-4781-8113-bfc1cd65f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(N, alpha, alpha_test, lambdaW, lambdaA, A_split, tol=(0.04, 150), patience=20, num_epochs=400, lr=0.01):\n",
    "    # require 4% improvement every 20 epochs\n",
    "    num_genes, num_tfs = N.shape\n",
    "    num_thetas = alpha.shape[1]\n",
    "    \n",
    "    model = TrainableModel(N, alpha, A_split, num_tfs, num_thetas, lambdaW, lambdaA)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_EV = -float(\"inf\")\n",
    "    best_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    test_EV_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       # Validation Step (keep computations in PyTorch)\n",
    "        W_sparse_vector = model.W.detach()\n",
    "        W_dense = torch.sparse_coo_tensor(\n",
    "            indices=torch.stack([model.i, model.j]),\n",
    "            values=torch.exp(W_sparse_vector),\n",
    "            size=(num_genes, num_tfs)\n",
    "        ).to_dense()\n",
    "        A = model.A.detach()\n",
    "\n",
    "        # Calculate R_test as a torch tensor\n",
    "        R_test = torch.matmul(W_dense, A)\n",
    "\n",
    "        # Calculate EV_test (convert tensors to NumPy for explained_variance_score)\n",
    "        EV_test = explained_variance_score(alpha_test.numpy(), R_test.detach().numpy())\n",
    "\n",
    "        # Calculate Test Loss in PyTorch\n",
    "        main_loss = torch.sum((alpha_test - R_test) ** 2)\n",
    "        l1_loss = torch.sum(torch.abs(W_sparse_vector))  # L1 on W\n",
    "        l2_loss = torch.sum(A ** 2)                      # L2 on the unconstrained A\n",
    "        total_test_loss = main_loss + lambdaW * l1_loss + lambdaA * l2_loss\n",
    "        \n",
    "        train_loss_history.append(loss.item())\n",
    "        test_loss_history.append(total_test_loss)\n",
    "        test_EV_history.append(EV_test)\n",
    "\n",
    "        # Check for early stopping\n",
    "        is_relative_loss_better = total_test_loss < best_test_loss * (1 - tol[0])\n",
    "        is_absolute_loss_better = total_test_loss < best_test_loss - tol[1]\n",
    "        if is_relative_loss_better and is_absolute_loss_better:\n",
    "            best_EV = EV_test\n",
    "            best_test_loss = total_test_loss\n",
    "            patience_counter = 0  # Reset patience\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.0f}, Loss test: {total_test_loss:.0f}, EV_test: {EV_test*100:.2f}%\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            if not is_relative_loss_better:\n",
    "                print(f\"Early stopping (relative={tol[0]}) at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            if not is_absolute_loss_better:\n",
    "                print(f\"Early stopping (absolute={tol[1]}) at epoch {epoch+1}. Best Loss test: {best_test_loss:.0f}. Best EV_test: {best_EV*100:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(4, 3))\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss value')\n",
    "    ax1.plot(range(epoch+1), train_loss_history, color='tab:red', label=\"Train Loss\")\n",
    "    ax1.plot(range(epoch+1), test_loss_history, color='tab:green', label=\"Test Loss\")\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend(loc='center right')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('EV Test', color='tab:blue')\n",
    "    ax2.plot(range(epoch+1), test_EV_history, color='tab:blue', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.suptitle('Loss and EV Over Epochs')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return W_dense.numpy(), A.numpy(), loss.item()#, total_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538973c-9628-4f67-8881-5642595f3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cross-Validation for Lambda Optimization\n",
    "def cross_val_lambda(N, alpha1, alpha2, lambdaW_values, lambdaA_values, A_split, A_split_2, tol=(0.04, 150), patience=20):\n",
    "    best_lambdaW, best_lambdaA, best_EV = None, None, -np.inf\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    EVs_avg = []\n",
    "\n",
    "    for lambdaW, lambdaA in product(lambdaW_values, lambdaA_values):\n",
    "        print(f\"Testing lambdaW = {lambdaW:.2f}, lambdaA = {lambdaA:.2f}\")\n",
    "\n",
    "        # Train on alpha1, test on alpha2\n",
    "        W1, A1, loss1 = train_model(N, alpha1, alpha2, lambdaW, lambdaA, A_split, tol, patience)\n",
    "        losses1.append(loss1)\n",
    "        R_test1 = W1 @ A1\n",
    "        EV1 = explained_variance_score(alpha2, R_test1)\n",
    "\n",
    "        # Train on alpha2, test on alpha1\n",
    "        W2, A2, loss2 = train_model(N, alpha2, alpha1, lambdaW, lambdaA, A_split_2, tol, patience)\n",
    "        losses2.append(loss2)\n",
    "        R_test2 = W2 @ A2\n",
    "        EV2 = explained_variance_score(alpha1, R_test2)\n",
    "\n",
    "        avg_EV = (EV1 + EV2) / 2\n",
    "        EVs_avg.append(avg_EV)\n",
    "        print(f\"lambdaW={lambdaW:.2f}, lambdaA={lambdaA:.2f}, EV={avg_EV*100:.2f}%\\n\")\n",
    "\n",
    "        if avg_EV > best_EV:\n",
    "            best_lambdaW, best_lambdaA, best_EV = lambdaW, lambdaA, avg_EV\n",
    "            \n",
    "    EV_surface = np.array(EVs_avg).reshape(len(lambdaW_values), len(lambdaA_values))\n",
    "    LambdaW, LambdaA = np.meshgrid(lambdaW_values, lambdaA_values, indexing='ij')\n",
    "\n",
    "    # Find optimal point\n",
    "    best_idx = np.unravel_index(np.argmax(EV_surface), EV_surface.shape)\n",
    "    opt_lambdaW = lambdaW_values[best_idx[0]]\n",
    "    opt_lambdaA = lambdaA_values[best_idx[1]]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    cp = plt.contourf(LambdaW, LambdaA, EV_surface, levels=30, cmap='viridis')\n",
    "    plt.colorbar(cp, label='Average Explained Variance')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('λ_W (W regularization)')\n",
    "    plt.ylabel('λ_A (A regularization)')\n",
    "    plt.title('EV Surface over λ_W, λ_A')\n",
    "    plt.scatter([opt_lambdaW], [opt_lambdaA], color='red', label='Optimum')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', ls='--', lw=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best λ_W = {best_lambdaW:.2f}, Best λ_A = {best_lambdaA:.2f}, Best EV = {best_EV*100:.2f}%\\n\")\n",
    "    return best_lambdaW, best_lambdaA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0918370-967c-4196-9a19-49d494d0f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cross train for best model\n",
    "def cross_train(N, alpha1, alpha2, best_lambdaW, best_lambdaA, A_split, A_split_2, tol=(0.04, 150), patience=20):\n",
    "    print(\"Training on α1, testing on α2...\")\n",
    "    W1, A1, loss1 = train_model(N, alpha1, alpha2, best_lambdaW, best_lambdaA, A_split, tol, patience)\n",
    "    R_test1 = W1 @ A1\n",
    "    EV1_train = explained_variance_score(alpha1.numpy(), R_test1)\n",
    "    EV1_test = explained_variance_score(alpha2.numpy(), R_test1)\n",
    "\n",
    "    print(\"Training on α2, testing on α1...\")\n",
    "    W2, A2, loss2 = train_model(N, alpha2, alpha1, best_lambdaW, best_lambdaA, A_split_2, tol, patience)\n",
    "    R_test2 = W2 @ A2\n",
    "    EV2_train = explained_variance_score(alpha2.numpy(), R_test2)\n",
    "    EV2_test = explained_variance_score(alpha1.numpy(), R_test2)\n",
    "\n",
    "    avg_EV_train = (EV1_train + EV2_train) / 2\n",
    "    avg_EV_test = (EV1_test + EV2_test) / 2\n",
    "    print(f\"Average EV_train: {avg_EV_train*100:.2f}%\")\n",
    "    print(f\"Average EV_test: {avg_EV_test*100:.2f}%\")\n",
    "    \n",
    "    return W1, A1, W2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a259a-0778-488f-b149-59970090e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_amplitudes(matrices_from, matrices_to = False, target_amp=0.2):\n",
    "    \"\"\"\n",
    "    Rescale the matrices to a single target amplitude.\n",
    "    \"\"\"\n",
    "    standardized_matrices_from = []\n",
    "    standardized_matrices_to = []\n",
    "    for i in range(len(matrices_from)):\n",
    "        amp = (np.max(matrices_from[i], axis=1) - np.min(matrices_from[i], axis=1)) / 2\n",
    "        scale = target_amp / amp\n",
    "        standardized_matrix_from = matrices_from[i] * scale[:, np.newaxis] #TF x theta\n",
    "        standardized_matrices_from.append(standardized_matrix_from)\n",
    "        if (matrices_to != False):\n",
    "            standardized_matrix_to = matrices_to[i] / scale[np.newaxis, :] #genes x TF\n",
    "            standardized_matrices_to.append(standardized_matrix_to)\n",
    "    \n",
    "    return standardized_matrices_from, standardized_matrices_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42624f9-10c9-4c9b-abd4-c4d2adcf220e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data & Run\n",
    "fileAlpha1 = \"alpha_snrna_rep1_5000_1_2p75.csv\"\n",
    "fileAlpha2 = \"alpha_snrna_rep2_5000_1_2p75.csv\"\n",
    "fileBSM = 'data_binding_site_matrix.txt'\n",
    "process = [\"transcription\", \"\\u03B1\"]\n",
    "theta_smooth = np.round(np.linspace(0.01, 1.00, 100), 2)  # 100 bins from 0.01 to 1.00\n",
    "ampl_threshold=0.1\n",
    "\n",
    "# Define Lambda Values\n",
    "lambdaW_values = np.logspace(-3, -1, 10)\n",
    "lambdaA_values = np.logspace(1, 2, 10)\n",
    "#best_lambdaW, best_lambdaA = 0, 0\n",
    "best_lambdaW, best_lambdaA = 0.0215, 77.43\n",
    "\n",
    "N = pd.read_csv(fileBSM, sep=\"\\t\",index_col=0)\n",
    "tf_names = N.columns\n",
    "alpha1 = pd.read_csv(fileAlpha1, sep=\",\",index_col=0)\n",
    "alpha2 = pd.read_csv(fileAlpha2, sep=\",\",index_col=0)\n",
    "#A_split = np.load(\"activities_export/ampl_\"+str(ampl_threshold)+\"/A_star_split.npy\")\n",
    "\n",
    "#Select common genes and normalize\n",
    "print(alpha1.shape, alpha2.shape, N.shape)\n",
    "alpha1_norm, alpha2_norm, N_norm, targetnames, tf_names = preprocessing(alpha1, alpha2, N, ampl_threshold=ampl_threshold)\n",
    "print(alpha1_norm.shape, alpha2_norm.shape, N_norm.shape, \"\\n\")\n",
    "\n",
    "n_runs = 5\n",
    "A1_list = []\n",
    "W1_list = []\n",
    "A2_list = []\n",
    "W2_list = []\n",
    "A1_split_list = []\n",
    "A2_split_list = []\n",
    "\n",
    "for seed in np.random.randint(1,100, n_runs):\n",
    "    torch.manual_seed(seed) #useless now that init is not random anymore\n",
    "    np.random.seed(seed)\n",
    "    lambdas = np.logspace(0, 6, 40)\n",
    "    A_split, lambda_opt1 = svd_regression_with_lambda_CV(alpha1_norm, N_norm, lambdas, seed=seed)\n",
    "    A_split_2, lambda_opt2 = svd_regression_with_lambda_CV(alpha2_norm, N_norm, lambdas, seed=seed)\n",
    "    A1_split_list.append(A_split)\n",
    "    A2_split_list.append(A_split_2)\n",
    "    #print(\"Best lambda:\", lambda_opt1)\n",
    "    #print(\"Best lambda:\", lambda_opt2)\n",
    "\n",
    "    N_tensor = torch.tensor(N_norm, dtype=torch.float32)  # (genes, TFs)\n",
    "    alpha1_tensor = torch.tensor(alpha1_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "    alpha2_tensor = torch.tensor(alpha2_norm, dtype=torch.float32)  # (genes, thetas)\n",
    "\n",
    "    tol = (0.03, 200) #(0.07, 800) for ampl 0.2  #np.linspace(0.01, 0.07, 7)\n",
    "    patience = 20 #np.linspace(5, 30, 7)\n",
    "    # Optimize Lambda\n",
    "    if (best_lambdaW == 0):\n",
    "        best_lambdaW, best_lambdaA = cross_val_lambda(N_tensor, alpha1_tensor, alpha2_tensor, lambdaW_values, lambdaA_values, A_split, A_split_2, tol=(0.04, 150), patience=20)\n",
    "    # Train and Cross-Test\n",
    "    print(\"Seed :\", seed, \"Tolerance :\", tol, \"Patience :\", patience)\n",
    "    W1, A1, W2, A2 = cross_train(N_tensor, alpha1_tensor, alpha2_tensor, best_lambdaW, best_lambdaA, A_split, A_split_2, tol, patience)\n",
    "    A1_list.append(A1)\n",
    "    W1_list.append(W1)\n",
    "    A2_list.append(A2)\n",
    "    W2_list.append(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88b3cc-f178-4827-a6f9-d3bdfe55adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tfs = A1_list[0].shape[0]\n",
    "n_genes = W1_list[0].shape[0]\n",
    "A_list = A1_list\n",
    "W_list = W1_list\n",
    "A_split_list = A1_split_list\n",
    "\n",
    "heatmap_vals = np.zeros((4, n_runs+1, n_runs+1)) #A_split  A1_1  A1_2  A1_3  A1_4  A1_5\n",
    "heatmap_labels = ['Model 1']\n",
    "for l in range(n_runs):\n",
    "    heatmap_labels.append(\"Rep \"+str(l+1))\n",
    "\n",
    "for k in range(n_runs+1): #Diagonal to 1\n",
    "    heatmap_vals[0][k, k] = 1.0\n",
    "    heatmap_vals[1][k, k] = 1.0\n",
    "    heatmap_vals[2][k, k] = 1.0\n",
    "    heatmap_vals[3][k, k] = 1.0\n",
    "\n",
    "for i, j in combinations(range(n_runs), 2): #Corr of A1 between runs\n",
    "    A1_i, A1_j = A_list[i], A_list[j]\n",
    "    W1_i, W1_j = W_list[i], W_list[j]\n",
    "\n",
    "    # Per TF correlation of activities\n",
    "    A_corrs = np.zeros(n_tfs)\n",
    "    for m in range(n_tfs):\n",
    "        A_corrs[m], _ = pearsonr(A1_i[m, :], A1_j[m, :])\n",
    "\n",
    "    # Per gene correlation of weights\n",
    "    W_corrs = np.zeros(n_genes)\n",
    "    for g in range(n_genes):\n",
    "        W_corrs[g], _ = pearsonr(W1_i[g, :], W1_j[g, :])\n",
    "        \n",
    "    heatmap_vals[0][i+1, j+1] = heatmap_vals[0][j+1, i+1] = round(np.mean(A_corrs), 3)\n",
    "    heatmap_vals[1][i+1, j+1] = heatmap_vals[1][j+1, i+1] = round(np.median(A_corrs), 3)\n",
    "    heatmap_vals[2][i+1, j+1] = heatmap_vals[2][j+1, i+1] = round(np.mean(W_corrs), 3)\n",
    "    heatmap_vals[3][i+1, j+1] = heatmap_vals[3][j+1, i+1] = round(np.median(W_corrs), 3)\n",
    "    \n",
    "for l in range(n_runs): #A_split corr with corresponding A after run\n",
    "    A_split, A1 = A_split_list[l], A_list[l]\n",
    "    W1 = W_list[l]\n",
    "\n",
    "    # Per TF correlation of activities\n",
    "    A_corrs = np.zeros(n_tfs)\n",
    "    for m in range(n_tfs):\n",
    "        A_corrs[m], _ = pearsonr(A_split[m, :], A1[m, :])\n",
    "\n",
    "    # Per gene correlation of weights\n",
    "    W_corrs = np.zeros(n_genes)\n",
    "    for g in range(n_genes):\n",
    "        W_corrs[g], _ = pearsonr(N_norm[g, :], W1[g, :])\n",
    "        \n",
    "    heatmap_vals[0][l+1, 0] = heatmap_vals[0][0, l+1] = round(np.mean(A_corrs), 3)\n",
    "    heatmap_vals[1][l+1, 0] = heatmap_vals[1][0, l+1] = round(np.median(A_corrs), 3)\n",
    "    heatmap_vals[2][l+1, 0] = heatmap_vals[2][0, l+1] = round(np.mean(W_corrs), 3)\n",
    "    heatmap_vals[3][l+1, 0] = heatmap_vals[3][0, l+1] = round(np.median(W_corrs), 3)\n",
    "\n",
    "for i, title in enumerate([\"Mean A Corr\", \"Median A Corr\", \"Mean W Corr\", \"Median W Corr\"]):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(heatmap_vals[i], cbar=True, annot=True, fmt=\".2f\", vmin=0, vmax=1, yticklabels=heatmap_labels,xticklabels=heatmap_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec879d50-42b6-4157-af87-74fede0ebd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_nb = np.where(tf_names == 'E2f1')[0][0]\n",
    "plot_binding_protein_activity(tf_names, A_split, process, theta_smooth, BP_nb=BP_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321b488-7cc4-48d7-8c42-3cd5f5d2aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_split = N_norm @ A_split\n",
    "explained_variance_score(alpha1_norm, R_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b07ef0-741d-404b-a89d-19eb70e83f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 13,          # base font size\n",
    "    'axes.labelsize': 13,     # axis label font size\n",
    "    'axes.titlesize': 11,     \n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 11,\n",
    "})\n",
    "plt.figure(figsize=(7,5))\n",
    "# Compute TF importance: sum of absolute weights per TF\n",
    "A1_list_standardized, W1_list_standardized = standardize_amplitudes(A1_list, W1_list, target_amp=0.2)\n",
    "print(\"tol=\", tol, \"ampl=\", ampl_threshold, \"Patience=\", patience)\n",
    "W = W1_list_standardized[0]\n",
    "TFs_sumW_df = W_key_TF(W, tf_names, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b3652-b3b9-4543-9210-cf171d08e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gene importance: sum of weights per gene\n",
    "Genes_sumW_df = W_key_gene(W, targetnames, top_k=20)\n",
    "#Genes_sumW_df.to_csv(\"mESC_genes_sumW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b92475-cdc1-467f-a692-bdec3c7508e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_targets(W, tf_names, gene_names, tf_query, top_k=20):\n",
    "    # Find TF index\n",
    "    tf_idx = np.where(tf_names == tf_query)[0][0]\n",
    "    W_tf = W[:, tf_idx]\n",
    "\n",
    "    # Filter out zero weights\n",
    "    nonzero_idx = np.where(W_tf != 0)[0]\n",
    "    W_tf_nonzero = W_tf[nonzero_idx]\n",
    "    gene_names_nonzero = [gene_names[i] for i in nonzero_idx]\n",
    "\n",
    "    # Sort by absolute weight (descending)\n",
    "    sorted_idx = np.argsort(W_tf_nonzero)[::-1]\n",
    "    top_gene_names = [gene_names_nonzero[i] for i in sorted_idx]\n",
    "    weights = [W_tf_nonzero[i] for i in sorted_idx]\n",
    "    print(str(len(weights))+\" targets for \"+tf_query)\n",
    "    \n",
    "    df = pd.DataFrame({'sum_W': weights}, index=top_gene_names)\n",
    "    #df = df.sort_values('sum_W', ascending=False)\n",
    "    \n",
    "    # --- Step 3: Plot top_k TFs ---\n",
    "    top_df = df.head(top_k)\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.bar(np.arange(top_k), top_df['sum_W'], color='red')\n",
    "    plt.xticks(np.arange(top_k), top_df.index, rotation=45, ha='right')\n",
    "    plt.ylabel(\"Binding site counts (W)\")\n",
    "    #plt.title(f\"Top {top_k} TFs by ∑W (After amplitude standardization)\")\n",
    "    plt.axhline(0, color='black', linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Build dictionary\n",
    "    targets_dic = dict(zip(top_gene_names, weights))\n",
    "    return targets_dic\n",
    "\n",
    "targets_dic = get_tf_targets(W, tf_names, targetnames, tf_query=\"E2f1\")\n",
    "#targets_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2f9d3-a5c4-49ff-b001-ae88cce9281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(W1_list_standardized[0][W1_list_standardized[0] > 0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c657c6c-f6f3-417d-912f-33b93ef2dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, A2 = A1_list[0], A1_list[1]\n",
    "residuals = A1 - A2\n",
    "std_residuals = residuals.std(axis=1)\n",
    "\n",
    "# Compute per-TF Pearson correlations\n",
    "n_tfs = A1.shape[0]\n",
    "pearson_rs = np.array([pearsonr(A1[i, :], A2[i, :])[0] for i in range(n_tfs)])\n",
    "\n",
    "# Plot histogram of std residuals => check for biases\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(std_residuals, bins=30, color='steelblue', alpha=0.8)\n",
    "plt.xlabel(\"Std of Residuals (A1 - A2)\")\n",
    "plt.ylabel(\"Number of TFs\")\n",
    "plt.title(\"Distribution of Std Residuals per TF\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Pearson r vs Residual std => identify shape and/or amplitude disagreements\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(pearson_rs, std_residuals, alpha=0.8, edgecolor='k')\n",
    "plt.xlabel(\"Pearson Correlation (A1 vs A2)\")\n",
    "plt.ylabel(\"Std of Residuals (A1 - A2)\")\n",
    "plt.title(\"Per-TF Reproducibility: Shape vs Magnitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2d2d8-ca9a-4e86-8f34-f4590dd9d0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A1, A2 = A1_list[0], A1_list[1]\n",
    "W1, W2 = W1_list[0], W1_list[1]\n",
    "print(np.sort(W1.flatten()))\n",
    "print(np.sort(A1.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca290251-fbb1-4774-bcf0-c6ec3842b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We smooth activities\n",
    "#A1 = fourier_fit(A1, theta_smooth)\n",
    "#A2 = fourier_fit(A2, theta_smooth)\n",
    "R1 = W1 @ A1\n",
    "R2 = W2 @ A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffebd6e-344c-4d79-96d8-90644b61b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 22,          # base font size\n",
    "    'axes.labelsize': 20,     # axis label font size\n",
    "    'axes.titlesize': 20,     \n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790f728-2faa-4a73-b61c-c2b894386ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = []\n",
    "for k in range(alpha1_norm.shape[0]):\n",
    "    expl.append(explained_variance_score(alpha2_norm[k,:], R1[k,:]))\n",
    "expl = np.array(expl)\n",
    "expl_sorted = np.sort(expl)[:]\n",
    "plt.hist(expl_sorted, bins=150)\n",
    "plt.xlabel(\"Explained variance\")\n",
    "plt.xlim(-2,1)\n",
    "plt.ylabel(\"Number of genes\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Distribution of gene-wise EVs for transcription\")\n",
    "plt.show()\n",
    "print(explained_variance_score(alpha2_norm, R1))\n",
    "np.mean(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856477d-0341-4f02-a24a-a8efc228bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_nb = np.where(tf_names == 'E2f4')[0][0]\n",
    "#BP_nb = 27\n",
    "plot_binding_protein_activity(tf_names, A1, process, theta_smooth, BP_nb=BP_nb)\n",
    "print(f\"Positive W1 among target genes of {tf_names[BP_nb]} : {np.sum(W1[:, BP_nb] > 0)}/{np.sum(W1[:, BP_nb] != 0)} ({np.sum(W1[:, BP_nb] > 0)/np.sum(W1[:, BP_nb] != 0)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f1d17-c013-4f58-a781-45bb08294df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1\n",
    "n = np.where(targetnames == 'Arfgef3')[0][0]\n",
    "#n = 138\n",
    "print(\"Train\")\n",
    "plot_rate_comparison(targetnames, alpha1_norm, R_split, process, theta_smooth, target_nb=n)\n",
    "print(\"Test\")\n",
    "plot_rate_comparison(targetnames, alpha2_norm, R_split, process, theta_smooth, target_nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6363ee-7e07-468f-9c69-e4ecfae77c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2\n",
    "n = np.where(targetnames == 'Ankrd10')[0][0]\n",
    "#n = 4972\n",
    "print(\"Train\")\n",
    "plot_rate_comparison(targetnames, alpha1_norm, R1, process, theta_smooth, target_nb=n)\n",
    "print(\"Test\")\n",
    "plot_rate_comparison(targetnames, alpha2_norm, R1, process, theta_smooth, target_nb=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052ba94-df26-41ec-a73f-5529eb00443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_reproducibility(A1, A2, alpha1_norm, alpha2_norm, metric=\"TF activities\")\n",
    "compute_reproducibility(W1, W2, alpha1_norm, alpha2_norm, metric=\"W site counts\")\n",
    "#compute_reproducibility(R1, R2, alpha1_norm, alpha2_norm, metric=\"Reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a2abd-2bc7-4e31-87b9-471b35b17fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_heatmap(A_split, ylabels=tf_names, display_limit=25, cmap='RdBu_r', title=\" TF activities across the cell cycle\")\n",
    "#plot_heatmap(alpha1_norm, cmap='RdBu_r', title=\" transcription rates across the cell cycle\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,          # base font size\n",
    "    'axes.labelsize': 18,     # axis label font size\n",
    "    'axes.titlesize': 18,     \n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "})\n",
    "#plot_heatmap(alpha1_norm, cmap='RdBu_r', title=\"Gene transcription (scRNA-seq) across the cell cycle\")\n",
    "plot_heatmap(A_split, cmap='RdBu_r', title=\"TFs activities on transcription across the cell cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafd589-9889-40b9-a774-b3eb36e69736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of BPs activity along cell cycle (Export)\n",
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2_E2f5\", \"E2f3\", \"E2f4\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\", \"Elf3\", \"Tfap4\"]\n",
    "tf_displayed = plot_heatmap_list(A1, tf_names, key_tfs, clip=True)\n",
    "print(tf_displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6f18b-5998-433c-8b56-25c63496160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_activity = {\n",
    "    \"Smad3\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": False}, #True?\n",
    "    \"Hbp1\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": True},\n",
    "    \"E2f1\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": False},\n",
    "    \"E2f2_E2f5\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": False},\n",
    "    \"E2f3\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": False},\n",
    "    \"E2f4\": {\"ranges\": [(0.05, 0.25)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f6\": {\"ranges\": [(0.3, 1)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f7\": {\"ranges\": [(0.3, 1)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"E2f8\": {\"ranges\": [(0.3, 1)], \"inhibitory\": True}, #Not 100% confident\n",
    "    \"Sp1\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": False}, #Can be both?\n",
    "    \"Hes1\": {\"ranges\": [(0.15, 0.35)], \"inhibitory\": True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77982-f4ab-416c-903c-d232697c7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF EXPRESSION AND BIOLOGICAL MEANING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af104-51b4-4141-8bc8-7f8d57e71317",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tfs = [\"Smad3\", \"Hbp1\", \"E2f1\", \"E2f2\", \"E2f3\", \"E2f4\", \"E2f5\", \"E2f6\", \"E2f7\", \"E2f8\", \"Sp1\", \"Hes1\"]\n",
    "\n",
    "tf_names_filtered = np.array([tf for tf in key_tfs if tf in alpha1.index and tf in alpha2.index and tf in tf_names])\n",
    "print(\"TFs in common :\", str(len(tf_names_filtered))+\"/\"+str(len(key_tfs)))\n",
    "\n",
    "alpha1_f, alpha2_f = alpha1.loc[tf_names_filtered], alpha2.loc[tf_names_filtered]\n",
    "alpha1_n, alpha2_n = alpha1_f.to_numpy(), alpha2_f.to_numpy()\n",
    "\n",
    "#Standardize amplitudes\n",
    "#A_standard = A_standard - np.mean(A_standard, axis=1, keepdims=True)\n",
    "alpha_sn_n = (alpha1_n + alpha2_n) / 2\n",
    "alpha_sn_norm = alpha_sn_n - np.mean(alpha_sn_n, axis=1, keepdims=True) - np.mean(alpha_sn_n, axis=0, keepdims=True) + np.mean(alpha_sn_n)\n",
    "matrices_from, _ = standardize_amplitudes([alpha_sn_norm, A1])\n",
    "alpha_sn_norm, A_standard = matrices_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdbe85-5a95-4e27-b3c7-445dc41e0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.size': 16,          # base font size\n",
    "    'axes.labelsize': 15,     # axis label font size\n",
    "    'axes.titlesize': 15,     \n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'axes.spines.right': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55373359-4bd7-4b2a-a93a-b5ef3d62370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "z_vals = []\n",
    "for tf in range(len(tf_names_filtered)):\n",
    "    plot_TF_exp_activity(theta_smooth, alpha_sn_norm, A_standard, tf_names, tf_names_filtered, tf)\n",
    "    corr = spearmanr(alpha_sn_norm[tf], A_standard[list(tf_names).index(tf_names_filtered[tf])])[0]\n",
    "    action = \"activator\" if not expected_activity[tf_names_filtered[tf]][\"inhibitory\"] else \"inhibitor\"\n",
    "    if (action == \"inhibitor\"):\n",
    "        corr = -corr\n",
    "    print(f\"scRNA & A correlation : {corr:.3f} ({ action })\\n\")\n",
    "    z_val = compute_tf_activity_difference(A_standard[list(tf_names).index(tf_names_filtered[tf]), :], theta_smooth, expected_activity[tf_names_filtered[tf]][\"ranges\"], expected_activity[tf_names_filtered[tf]][\"inhibitory\"])\n",
    "    corrs.append(corr)\n",
    "    z_vals.append(z_val)\n",
    "    print(f\"Expected activity range : {expected_activity[tf_names_filtered[tf]][\"ranges\"]}\")\n",
    "    print(f\"TF activity biological z-score : {z_val:.2f} ({ action })\")\n",
    "print(f\"\\nGlobal correlation :{np.mean(corrs):.3f}\")\n",
    "print(f\"Global z-score :{np.mean(z_vals):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main 3.12.9",
   "language": "python",
   "name": "main_3_12_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
